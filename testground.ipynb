{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./data/demo.ply\n",
      "Read 942188 points\n",
      "Info: Generated by CloudCompare!\n",
      "Fields: vertex: x, y, z, red, green, blue, scalar_Original_cloud_index, \n",
      "DEBUG: init parameters\n",
      "Building KDTree\n",
      "DEBUG: subdivvide into chunks\n",
      "DEBUG: creating threads loop\n",
      "Chunk_id: 0\tNum. Points: 942188\n",
      "DEBUG: computing normals\n",
      "Starting cleanup.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import segmentation.reggrow.build.lib.reggrow as rg\n",
    "pcd = rg.PointCloud.from_ply(\"./data/demo.ply\", True)\n",
    "isinstance_inds = np.ones(pcd.size, dtype=np.int32) * -1\n",
    "rg.region_growing(\n",
    "    pcd,\n",
    "    isinstance_inds,\n",
    "    1,1,1,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./data/demo.ply\n",
      "Read 942188 points\n",
      "Info: Generated by CloudCompare!\n",
      "Fields: vertex: x, y, z, red, green, blue, scalar_Original_cloud_index, \n",
      "DEBUG: init parameters\n",
      "Building KDTree\n",
      "DEBUG: subdivvide into chunks\n",
      "DEBUG: creating threads loop\n",
      "Chunk_id: 0\tNum. Points: 942188\n",
      "DEBUG: computing normals\n",
      "Starting cleanup.\n"
     ]
    }
   ],
   "source": [
    "from annotation import IndexSegmentation\n",
    "\n",
    "reggrow_idx=0\n",
    "pcd = rg.PointCloud.from_ply(\"./data/demo.ply\", True)\n",
    "reggrow_segmentation = IndexSegmentation(f'region_growing_{reggrow_idx:03}', pcd.size)\n",
    "rg.region_growing(\n",
    "    pcd,\n",
    "    reggrow_segmentation.instance_idx,\n",
    "    1,1,1,\n",
    "    verbose=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "from torch_geometric.nn import knn_graph, radius_graph\n",
    "# sys.path.append(\"./parallel_cut_pursuit/pcd-prox-split/grid-graph/python\")\n",
    "sys.path.append(\"./segmentation/cutpursuit/pcd-prox-split/grid-graph/python\")\n",
    "from grid_graph import edge_list_to_forward_star # type: ignore\n",
    "from segmentation.cutpursuit.python.wrappers.cp_d0_dist import cp_d0_dist\n",
    "from grid_graph import edge_list_to_forward_star\n",
    "from plyfile import PlyData\n",
    "import pandas as pd\n",
    "from torch_scatter import scatter_add\n",
    "\n",
    "@torch.no_grad()\n",
    "def scatter_eigendecomposition(src: torch.Tensor, index: torch.Tensor, G: int = None, eps: float = 1e-6):\n",
    "    \"\"\"Compute per-point eigenvalues and eigenvectors of local covariance.\"\"\"\n",
    "    if G is None:\n",
    "        G = int(index.max().item()) + 1\n",
    "    N, D = src.shape\n",
    "    # accumulate counts\n",
    "    ones = torch.ones_like(index, dtype=src.dtype)\n",
    "    counts = scatter_add(ones, index, dim=0, dim_size=G)  # [G]\n",
    "    # mean\n",
    "    sum_src = scatter_add(src, index, dim=0, dim_size=G)   # [G, D]\n",
    "    mu = sum_src / (counts[:, None] + eps)\n",
    "    # second moment\n",
    "    x = src.unsqueeze(-1)                                 # [N, D, 1]\n",
    "    xxT = x @ x.transpose(1, 2)                           # [N, D, D]\n",
    "    xxT_flat = xxT.reshape(N, D*D)                        # [N, D^2]\n",
    "    sum_xxT_flat = scatter_add(xxT_flat, index, dim=0, dim_size=G)  # [G, D^2]\n",
    "    E_xxT = sum_xxT_flat.reshape(G, D, D) / (counts[:, None, None] + eps)\n",
    "    # covariance\n",
    "    mu_muT = mu.unsqueeze(-1) @ mu.unsqueeze(1)           # [G, D, D]\n",
    "    cov = E_xxT - mu_muT\n",
    "    cov = cov + eps * torch.eye(D, device=src.device).unsqueeze(0)\n",
    "    # eigendecompose\n",
    "    eigvals, eigvecs = torch.linalg.eigh(cov)             # eigvals [G, D], eigvecs [G, D, D]\n",
    "    return eigvals, eigvecs\n",
    "\n",
    "def compute_features(pos: np.ndarray, colors: np.ndarray, r=0.05, eps=1e-6):\n",
    "    \"\"\"Compute per-point geometric + color features.\"\"\"\n",
    "    pos_t = torch.from_numpy(pos).float()\n",
    "    # build k-NN for local neighborhoods\n",
    "    # edge_index = knn_graph(pos_t, k=knn_k)  # [2, E]\n",
    "    edge_index = radius_graph(pos_t, r=r)\n",
    "    src_idx = edge_index[0]\n",
    "    nbr_idx = edge_index[1]\n",
    "    # gather neighbor coordinates\n",
    "    src = pos_t[nbr_idx]  # [E, 3]\n",
    "    # scatter-eigendecompose: groups src by src_idx (center index)\n",
    "    eigvals, eigvecs = scatter_eigendecomposition(src, src_idx, G=pos_t.size(0), eps=eps)\n",
    "    # eigenvalues sorted ascending: λ1 ≤ λ2 ≤ λ3\n",
    "    λ1, λ2, λ3 = eigvals[:, 0], eigvals[:, 1], eigvals[:, 2]\n",
    "    # geometric features\n",
    "    linearity  = (λ3 - λ2) / (λ3 + eps)\n",
    "    planarity  = (λ2 - λ1) / (λ3 + eps)\n",
    "    scattering = λ1 / (λ3 + eps)\n",
    "    # verticality: abs of normal's Z component (normal = eigenvector of λ1)\n",
    "    normals = eigvecs[:, :, 0]  # [N, 3]\n",
    "    verticality = normals[:, 2].abs()\n",
    "    # color features normalized to [0,1]\n",
    "    color_t = torch.from_numpy(colors).float() / 255.0  # [N, 3]\n",
    "    # stack all features: [N, 7]\n",
    "    feats = torch.stack([linearity, planarity, scattering, verticality,\n",
    "                         color_t[:, 0], color_t[:, 1], color_t[:, 2]], dim=1)\n",
    "    return feats.cpu().numpy()\n",
    "\n",
    "# -------------------\n",
    "# Load point cloud\n",
    "# -------------------\n",
    "pcd = PlyData.read('./data/demo.ply').elements[0].data\n",
    "# pos = np.vstack([pcd['x'], pcd['y'], pcd['z']]).T.astype(np.float32)\n",
    "pcd_pd = pd.DataFrame(pcd)\n",
    "pos = pcd_pd[['x','y','z']].to_numpy()\n",
    "pos -= pos.mean(axis=0)\n",
    "pos = pos.astype(np.float32)\n",
    "colors = np.vstack([pcd['red'], pcd['green'], pcd['blue']]).T.astype(np.float32)\n",
    "\n",
    "# -------------------\n",
    "# Compute features\n",
    "# -------------------\n",
    "features = compute_features(pos, colors, r=0.05)\n",
    "\n",
    "# common settings\n",
    "n_dim = 3\n",
    "n_feat = features.shape[1]\n",
    "\n",
    "# Helper to build CSR\n",
    "def build_csr(edge_idx, num_nodes):\n",
    "    src_csr, tgt, reidx = edge_list_to_forward_star(num_nodes, edge_idx.T.contiguous().cpu().numpy())\n",
    "    return src_csr.astype(np.uint32), tgt.astype(np.uint32), reidx\n",
    "\n",
    "# first-level partition params (λ1 chosen so |P0|/|P1|≈30)\n",
    "reg1 = 1\n",
    "sw1  = 1.0  # spatial weight\n",
    "cut1 = 1    # min superpoint size\n",
    "\n",
    "# 1) Level-0 graph: use k-NN on raw points\n",
    "pos_t = torch.from_numpy(pos).float()\n",
    "edge0 = knn_graph(pos_t, k=10)\n",
    "edge0 = torch.cat([edge0, edge0.flip(0)], dim=1)  # symmetrize\n",
    "\n",
    "src0, tgt0, re0 = build_csr(edge0, pos.shape[0])\n",
    "ew0 = np.ones_like(tgt0, dtype=np.float64) * reg1\n",
    "vw0 = np.ones(pos.shape[0], dtype=np.float64)\n",
    "cw0 = np.ones(n_dim + n_feat, dtype=np.float64)\n",
    "cw0[:n_dim] *= sw1\n",
    "\n",
    "X0 = np.concatenate([pos - pos.mean(0), features], axis=1)\n",
    "X0f = np.asfortranarray(X0.T, dtype=np.float64)\n",
    "\n",
    "super0, x0c, clst0, edges0, times0 = cp_d0_dist(\n",
    "    n_dim+n_feat, X0f, src0, tgt0,\n",
    "    edge_weights=ew0,\n",
    "    vert_weights=vw0,\n",
    "    coor_weights=cw0,\n",
    "    min_comp_weight=cut1,\n",
    "    cp_dif_tol=1e-2,\n",
    "    cp_it_max=10,\n",
    "    split_damp_ratio=0.7,\n",
    "    verbose=False,\n",
    "    max_num_threads=0,\n",
    "    balance_parallel_split=True,\n",
    "    compute_Time=True,\n",
    "    compute_List=True,\n",
    "    compute_Graph=True\n",
    ")\n",
    "\n",
    "# -------------------\n",
    "# Prepare level-1 data\n",
    "# -------------------\n",
    "# x0c: [D, R0] -> spatial + feature\n",
    "D0 = n_dim + n_feat\n",
    "R0 = x0c.shape[1]\n",
    "# spatial coordinates\n",
    "pos1 = x0c[:n_dim].T + pos.mean(0)\n",
    "# aggregated features\n",
    "feat1 = x0c[n_dim:].T\n",
    "\n",
    "# second-level partition params (λ2 chosen so |P1|/|P2|≈5)\n",
    "reg2 = 0.5\n",
    "sw2  = 1.0\n",
    "cut2 = 1\n",
    "\n",
    "# build graph on superpoints (level1)\n",
    "pos1_t = torch.from_numpy(pos1).float()\n",
    "edge1 = knn_graph(pos1_t, k=10)\n",
    "edge1 = torch.cat([edge1, edge1.flip(0)], dim=1)\n",
    "\n",
    "src1, tgt1, re1 = build_csr(edge1, R0)\n",
    "ew1 = np.ones_like(tgt1, dtype=np.float64) * reg2\n",
    "vw1 = np.bincount(super0, minlength=R0).astype(np.float64)  # cluster sizes\n",
    "cw1 = np.ones(n_dim + n_feat, dtype=np.float64)\n",
    "cw1[:n_dim] *= sw2\n",
    "\n",
    "X1 = np.concatenate([pos1 - pos1.mean(0), feat1], axis=1)\n",
    "X1f = np.asfortranarray(X1.T, dtype=np.float64)\n",
    "\n",
    "# super1, x1c, clst1, edges1, times1 = cp_d0_dist(\n",
    "#     n_dim+n_feat, X1f, src1, tgt1,\n",
    "#     edge_weights=ew1,\n",
    "#     vert_weights=vw1,\n",
    "#     coor_weights=cw1,\n",
    "#     min_comp_weight=cut2,\n",
    "#     cp_dif_tol=1e-2,\n",
    "#     cp_it_max=10,\n",
    "#     split_damp_ratio=0.7,\n",
    "#     verbose=False,\n",
    "#     max_num_threads=0,\n",
    "#     balance_parallel_split=True,\n",
    "#     compute_Time=True,\n",
    "#     compute_List=True,\n",
    "#     compute_Graph=True\n",
    "# )\n",
    "\n",
    "# print(f\"Level-0 superpoints: {x0c.shape[1]}, Level-1 superpoints: {x1c.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u,c = np.unique(super0, return_counts=True)\n",
    "u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(266,)\n",
      "(670,)\n",
      "(670,)\n"
     ]
    }
   ],
   "source": [
    "print(edges0[0].shape)\n",
    "print(edges0[1].shape)\n",
    "print(edges0[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import colorsys\n",
    "\n",
    "def index_to_color(indices):\n",
    "    \"\"\"\n",
    "    Map integer indices to vibrant RGB colors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    indices : int or array-like of ints\n",
    "        Input index or indices. Can be a scalar or an iterable of ints.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    colors : ndarray of shape (N, 3)\n",
    "        RGB colors in the range [0, 1], where N is the number of indices.\n",
    "    \"\"\"\n",
    "    # Ensure array of ints\n",
    "    indices = np.atleast_1d(np.array(indices, dtype=int).flatten())\n",
    "    \n",
    "    # Use the golden ratio conjugate to spread hues evenly\n",
    "    phi = 0.618033988749895\n",
    "    hues = (indices * phi) % 1.0\n",
    "    \n",
    "    # Vibrancy settings\n",
    "    s, v = 0.9, 0.95  # saturation and value\n",
    "    \n",
    "    # Convert each hue to RGB\n",
    "    colors = np.array([colorsys.hsv_to_rgb(h, s, v) for h in hues], dtype=float)\n",
    "    return colors\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ead2322d1148479cd86be1e20db829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:43051/index.html?ui=P_0x7156c43cf500_2&reconnect=auto\" class=\"pyvi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyvista as pv\n",
    "colors = index_to_color(super0)\n",
    "\n",
    "plotter = pv.Plotter(notebook=True)\n",
    "\n",
    "poly = pv.PolyData(pos)\n",
    "poly['rgb'] = colors\n",
    "\n",
    "plotter.add_mesh(poly, scalars='rgb',rgb=True)\n",
    "\n",
    "plotter.show(jupyter_backend='trame')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import maxflow\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "\n",
    "class GraphCutDPGMM:\n",
    "    def __init__(self, features, edges, smoothing_beta=1.0, pairwise_lambda=1.0,\n",
    "                 dp_alpha=1.0, max_components=10):\n",
    "        self.features = features\n",
    "        self.V, self.F = features.shape\n",
    "        self.first_edge, self.adj_vertices = edges\n",
    "        self.beta =smoothing_beta\n",
    "        self.lmbda = pairwise_lambda\n",
    "        self.max_components = max_components\n",
    "\n",
    "        common_kwargs = dict(\n",
    "            weight_concentration_prior_type = 'dirichlet_process',\n",
    "            weight_concentration_prior=dp_alpha,\n",
    "            n_components=max_components,\n",
    "            covariance_type='full',\n",
    "            max_iter=100,\n",
    "            random_state=0\n",
    "        )\n",
    "        self.model_fg = BayesianGaussianMixture(**common_kwargs) # type:ignore\n",
    "        self.model_bg = BayesianGaussianMixture(**common_kwargs) # type:ignore\n",
    "        n_init = min(self.V, max_components)\n",
    "        init_indices = np.random.RandomState(0).choice(self.V, size=n_init, replace=False)\n",
    "        dummy = self.features[init_indices]\n",
    "        self.model_fg.fit(dummy)\n",
    "        self.model_bg.fit(dummy)\n",
    "\n",
    "    def update_clicks(self, pos_indices, neg_indices):\n",
    "        if len(pos_indices) > 0:\n",
    "            Xp = self.features[pos_indices]\n",
    "            k_fg = min(len(pos_indices), self.max_components)\n",
    "            self.model_fg.set_params(n_components=k_fg)\n",
    "            self.model_fg.fit(Xp)\n",
    "\n",
    "        if len(neg_indices) > 0:\n",
    "            Xn = self.features[neg_indices]\n",
    "            k_bg = min(len(neg_indices), self.max_components)\n",
    "            self.model_bg.set_params(n_components=k_bg)\n",
    "            self.model_bg.fit(Xn)\n",
    "\n",
    "    def compute_unary(self):\n",
    "        logp_fg = self.model_fg.score_samples(self.features)\n",
    "        logp_bg = self.model_bg.score_samples(self.features)\n",
    "        cost_source = -logp_fg  # cost source[u]    = -log P(x_u | fg)\n",
    "        cost_sink   = -logp_bg  # cost_sink[u]      = -log P(x_u | bg)\n",
    "\n",
    "        return cost_source, cost_sink\n",
    "    \n",
    "    def build_graph(self, cost_source, cost_sink):\n",
    "        g = maxflow.Graph[float](self.V, self.adj_vertices.size)\n",
    "        nodeids = g.add_nodes(self.V)\n",
    "        # t-links\n",
    "        for u in range(self.V):\n",
    "            g.add_tedge(u, cost_source[u], cost_sink[u])\n",
    "        # pairwise edges\n",
    "        fe, av = self.first_edge, self.adj_vertices\n",
    "        for u in range(self.V):\n",
    "            start, end = fe[u], fe[u+1]\n",
    "            fu = self.features[u]\n",
    "            for idx in range(start, end):\n",
    "                v = av[idx]\n",
    "                # weight = lambda * exp(-beta * ||f_u - f_v||^2)\n",
    "                diff = fu - self.features[v]\n",
    "                w = self.lmbda * np.exp(-self.beta * (diff @ diff))\n",
    "                g.add_edge(u,v,w,w)\n",
    "        return g\n",
    "    \n",
    "    def solve(self):\n",
    "        cs, ct = self.compute_unary()\n",
    "        g = self.build_graph(cs, ct)\n",
    "        flow = g.maxflow()\n",
    "        labels = np.array([g.get_segment(u) for u in range(self.V)])\n",
    "        return labels.astype(bool)\n",
    "    \n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([135, 135, 135, ...,  41,  41,  41], shape=(942188,), dtype=uint32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "super0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_scatter import scatter_mean\n",
    "\n",
    "G = super0.max()+1\n",
    "evals, evecs = scatter_eigendecomposition(\n",
    "    src = torch.as_tensor(pos-pos.mean(axis=0),dtype=torch.float32),\n",
    "    index = torch.as_tensor(super0,dtype=torch.int64),\n",
    "    G = G\n",
    ")\n",
    "\n",
    "eps = 1e-5\n",
    "# eigenvalues sorted ascending: λ1 ≤ λ2 ≤ λ3\n",
    "l1, l2, l3 = evals[:, 0], evals[:, 1], evals[:, 2]\n",
    "# geometric features\n",
    "linearity  = (l3 - l2) / (l3 + eps)\n",
    "planarity  = (l2 - l1) / (l3 + eps)\n",
    "scattering = l1 / (l3 + eps)\n",
    "\n",
    "normals = evecs[:, :, 0]  # [N, 3]\n",
    "verticality = normals[:, 2].abs()\n",
    "\n",
    "mean_color = scatter_mean(\n",
    "    torch.as_tensor(colors,dtype=torch.float32),\n",
    "    torch.as_tensor(super0,dtype=torch.int64), dim=0, \n",
    "    dim_size=G)\n",
    "\n",
    "# stack all features: [N, 7]\n",
    "feats = torch.stack([linearity, planarity, scattering, verticality,\n",
    "                        mean_color[:, 0], mean_color[:, 1], mean_color[:, 2]], dim=1).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc = GraphCutDPGMM(feats, (edges0[0],edges0[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected n_samples >= n_components but got n_components = 10, n_samples = 3",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m pos_clicks = [\u001b[32m10\u001b[39m, \u001b[32m20\u001b[39m, \u001b[32m30\u001b[39m]\n\u001b[32m      2\u001b[39m neg_clicks = [\u001b[32m100\u001b[39m, \u001b[32m110\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mgc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate_clicks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_clicks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_clicks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m fg_mask = gc.solve()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mGraphCutDPGMM.update_clicks\u001b[39m\u001b[34m(self, pos_indices, neg_indices)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pos_indices) >= \u001b[32m2\u001b[39m:\n\u001b[32m     44\u001b[39m     Xp = \u001b[38;5;28mself\u001b[39m.features[pos_indices]\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_fg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(neg_indices) >= \u001b[32m2\u001b[39m:\n\u001b[32m     47\u001b[39m     Xn = \u001b[38;5;28mself\u001b[39m.features[neg_indices]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3dapp/lib/python3.12/site-packages/sklearn/mixture/_base.py:180\u001b[39m, in \u001b[36mBaseMixture.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Estimate model parameters with the EM algorithm.\u001b[39;00m\n\u001b[32m    155\u001b[39m \n\u001b[32m    156\u001b[39m \u001b[33;03mThe method fits the model ``n_init`` times and sets the parameters with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    177\u001b[39m \u001b[33;03m    The fitted mixture.\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# parameters are validated in fit_predict\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3dapp/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3dapp/lib/python3.12/site-packages/sklearn/mixture/_base.py:213\u001b[39m, in \u001b[36mBaseMixture.fit_predict\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    211\u001b[39m X = validate_data(\u001b[38;5;28mself\u001b[39m, X, dtype=[np.float64, np.float32], ensure_min_samples=\u001b[32m2\u001b[39m)\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X.shape[\u001b[32m0\u001b[39m] < \u001b[38;5;28mself\u001b[39m.n_components:\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    214\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExpected n_samples >= n_components \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    215\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbut got n_components = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.n_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    216\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mn_samples = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    217\u001b[39m     )\n\u001b[32m    218\u001b[39m \u001b[38;5;28mself\u001b[39m._check_parameters(X)\n\u001b[32m    220\u001b[39m \u001b[38;5;66;03m# if we enable warm_start, we will have a unique initialisation\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Expected n_samples >= n_components but got n_components = 10, n_samples = 3"
     ]
    }
   ],
   "source": [
    "pos_clicks = [10, 20, 30]\n",
    "neg_clicks = [100, 110]\n",
    "\n",
    "gc.update_clicks(pos_clicks, neg_clicks)\n",
    "fg_mask = gc.solve()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3dapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
