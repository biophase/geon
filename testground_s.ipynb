{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plyfile import PlyData, PlyElement\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyvista as pv\n",
    "import sys\n",
    "import os.path as osp\n",
    "from matplotlib import pyplot as plt\n",
    "sys.path.append(\"./segmentation/cutpursuit/pcd-prox-split/grid-graph/python\")\n",
    "from grid_graph import edge_list_to_forward_star # type: ignore\n",
    "from segmentation.cutpursuit.python.wrappers.cp_d0_dist import cp_d0_dist\n",
    "\n",
    "\n",
    "\n",
    "# pcd_fp = \"./data/flatLabels/flatLabels_viadukt.ply\"\n",
    "# pcd_fp = \"./data/flatLabels/flatLabels_twingen.ply\"\n",
    "# pcd_fp = \"./data/flatLabels/flatLabels_westbahnhof.ply\"\n",
    "pcd_fp = \"./data/flatLabels/flatLabels_niebelungen.ply\"\n",
    "\n",
    "\n",
    "pcd = pd.DataFrame(PlyData.read(pcd_fp).elements[0].data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(osp.join(osp.split(pcd_fp)[0],osp.basename(pcd_fp).replace('.ply','.yaml')),'r') as f:\n",
    "    label_schema = yaml.safe_load(f)\n",
    "\n",
    "label_map = {\n",
    "    'flatLabels_westbahnhof.ply' : \n",
    "        np.array([\n",
    "            [0, 0], #'cable',\n",
    "            [1, 0], #'concrete_beam',\n",
    "            [2, 0], #'concrete_cover',\n",
    "            [3, 1], #'concrete_cover_damaged',\n",
    "            [4, 0], #'efflorecence',\n",
    "            [5, 0], #'graffiti',\n",
    "            [6, 0], #'lamps',\n",
    "            [7, 0], #'reinforcment',\n",
    "            [8, 0], #'spalling'\n",
    "        ]),\n",
    "    'flatLabels_twingen.ply':\n",
    "        np.array([\n",
    "            [0, 0], #'paving',\n",
    "            [1, 0], #'pier_cap',\n",
    "            [2, 0], #'pipe',\n",
    "            [3, 0], #'pipe_fasteners',\n",
    "            [4, 0], #'railing',\n",
    "            [5, 0], #'unspecified',\n",
    "            [6, 0], #'vegetation',\n",
    "            [7, 0], #'scan_artefact',\n",
    "            [8, 0], #'road_barrier',\n",
    "            [9, 0], #'kerbstone',\n",
    "            [10, 0], #'deck',\n",
    "            [11, 1], #'roadway',\n",
    "            [12, 0], #'road_marking',\n",
    "            [13, 0], #'column',\n",
    "            [14, 0], #'natural_ground',\n",
    "        ]),\n",
    "    'flatLabels_viadukt.ply':\n",
    "        np.array([\n",
    "            [0, 0], # concrete\n",
    "            [1, 1], # brick\n",
    "        ]),\n",
    "    'flatLabels_niebelungen.ply':\n",
    "        np.array([\n",
    "            [0, 0], # 'addon',\n",
    "            [1, 0], # 'attatchment',\n",
    "            [2, 0], # 'beam',\n",
    "            [3, 1], # 'column',\n",
    "            [4, 0], # 'deck',\n",
    "            [5, 0], # 'fundament',\n",
    "            [6, 0], # 'pipe',\n",
    "            [7, 0], # 'railing',\n",
    "            [8, 0], # 'roadway',\n",
    "            [9, 0], # 'road_barrier',\n",
    "            [10, 0], # 'road_marking',\n",
    "            [11, 0], # 'scan_artefact',\n",
    "        ]),\n",
    "\n",
    "}\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd.columns\n",
    "field_map = {\n",
    "    'flatLabels_westbahnhof.ply':{\n",
    "        'x' : 'x',\n",
    "        'y' : 'y',\n",
    "        'z' : 'z',\n",
    "        'red' : 'red',\n",
    "        'green' : 'green',\n",
    "        'blue' : 'blue',\n",
    "        'riegl_reflectance' : 'riegl_reflectance',\n",
    "        'riegl_deviation' : 'riegl_deviation',\n",
    "        'labels' : 'labels',\n",
    "    },\n",
    "    'flatLabels_twingen.ply':{\n",
    "        'x':'x',\n",
    "        'y':'y',\n",
    "        'z':'z',\n",
    "        'red':'red',\n",
    "        'green':'green',\n",
    "        'blue':'blue',\n",
    "        'intensity':'intensity',\n",
    "        'labels':'labels',\n",
    "    },\n",
    "    'flatLabels_viadukt.ply':{\n",
    "        'x' : 'x',\n",
    "        'y' : 'y',\n",
    "        'z' : 'z',\n",
    "        'red' : 'red',\n",
    "        'green' : 'green',\n",
    "        'blue' : 'blue',\n",
    "        'scalar_material' : 'labels',\n",
    "    },\n",
    "    'flatLabels_niebelungen.ply':{\n",
    "        'x' : 'x',\n",
    "        'y' : 'y',\n",
    "        'z' : 'z',\n",
    "        'red' : 'red',\n",
    "        'green' : 'green',\n",
    "        'blue' : 'blue',\n",
    "        'Intensity' : 'intensity',\n",
    "        'labels' : 'labels',\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and remap fields\n",
    "\n",
    "pos = pcd[['x','y','z']].to_numpy().astype(np.float32)\n",
    "rgb = pcd[['red','green','blue']].to_numpy().astype(np.float32) if 'red' in pcd.columns else np.zeros((pos.shape[0],3), np.float32)\n",
    "intensity = pcd['intensity'].to_numpy().astype(np.float32) if 'intensity' in pcd.columns else np.zeros(pos.shape[0], np.float32)\n",
    "label = pcd['labels'].to_numpy().astype(int) if 'labels' in pcd.columns else pcd['scalar_material'].to_numpy().astype(int)\n",
    "\n",
    "fn = osp.basename(pcd_fp)\n",
    "# remap labels\n",
    "label_bin = label_map[fn][:,1][label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset camera_positions dict\n",
    "\n",
    "camera_positions = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load camera positions dict\n",
    "import pickle\n",
    "with open('./data/flatLabels/camera_positions.pkl','rb') as f:\n",
    "    camera_positions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # capture camera position in a dict\n",
    "# import os.path as osp\n",
    "# current_camera_position = plotter.camera_position\n",
    "# camera_positions[osp.basename(pcd_fp)] = current_camera_position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save camera_pos_dict\n",
    "# import pickle\n",
    "# with open('./data/flatLabels/camera_positions.pkl','wb') as f:\n",
    "#     pickle.dump(camera_positions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset camera position\n",
    "def reset_camera_standard(plotter:pv.Plotter):\n",
    "    standard_view = camera_positions[osp.basename(pcd_fp)]\n",
    "    plotter.camera_position = standard_view\n",
    "    plotter.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize loaded file\n",
    "\n",
    "plotter = pv.Plotter(notebook=True)\n",
    "pcd_poly = pv.PolyData(pos)\n",
    "pcd_poly['RGB']=rgb\n",
    "pcd_poly['label_bin']=label_bin\n",
    "# plotter.add_mesh(pcd_poly,scalars='RGB',rgb=True)\n",
    "plotter.add_mesh(pcd_poly,scalars='label_bin')\n",
    "plotter.show(jupyter_backend='trame')\n",
    "reset_camera_standard(plotter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation.interactive_gc import scatter_eig\n",
    "from torch_geometric.nn import knn_graph\n",
    "from typing import Optional, Tuple\n",
    "from torch_scatter import scatter_add\n",
    "import torch\n",
    "@torch.no_grad()\n",
    "def scatter_eigendecomposition(src:torch.Tensor, index: torch.Tensor, G:Optional[int]=None, eps:float=1e-6)->Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Performs eigendecomposition on subsets of `src` defined by `index`\n",
    "\n",
    "    Args:\n",
    "        src (torch.Tensor): Source data [N,D]\n",
    "        index (torch.Tensor): Index into output [N,]->[0..G]\n",
    "        G (int): Sets the output size explicitly, defaults to max(index) +1\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor,torch.Tensor]: sorted eigenvalues [G,D] and corresponding eigenvectors [G,D]\n",
    "    \"\"\"\n",
    "    \n",
    "    # _, counts = torch.unique(index, return_counts=True)\n",
    "    if G is None:\n",
    "        G = int(index.amax().item()) + 1\n",
    "    \n",
    "    N, D = src.shape\n",
    "    counts = torch.bincount(index, minlength=G)\n",
    "    \n",
    "    # print(f'old G={int(index.max().item()) + 1}, WhyNotThisG = {counts.size(0)}')\n",
    "    # if (int(index.max().item()) + 1 != counts.size(0)):\n",
    "    #     raise Exception\n",
    "\n",
    "\n",
    "    ones    = torch.ones_like(index, dtype=src.dtype)\n",
    "    counts  = scatter_add(ones, index, dim=0, dim_size=G)  # [G]\n",
    "\n",
    "\n",
    "\n",
    "    sum_src = scatter_add(src, index, dim=0, dim_size=G)        # [G,D]\n",
    "    mu = sum_src / (counts[:,None]+eps)                         # [G,D]\n",
    "\n",
    "    x = src [...,None]              # [N,D,1]\n",
    "    xxT = x @ x.permute(0,2,1)      # [N,D,D]\n",
    "    xxT_flat = xxT.reshape(N,D*D)   # [N,D^2]\n",
    "\n",
    "    sum_xxT_flat = scatter_add(xxT_flat, index, dim=0, dim_size=G)      # [G,D^2]\n",
    "    E_xxT = sum_xxT_flat / (counts[:,None]+eps)                         # [G,D,D]\n",
    "    E_xxT = E_xxT.reshape(G, D, D)\n",
    "\n",
    "\n",
    "    mu_muT = mu[...,None] @ mu[:,None,:]    # [G,D,1] @ [G,!,D] -> [G,D,D]\n",
    "    cov = E_xxT - mu_muT                    # [G,D,D]\n",
    "    \n",
    "    # Ridge (Tikhonov)-regularization\n",
    "    eye = torch.eye(D, device=src.device).unsqueeze(0)  # [1,D,D]\n",
    "    cov = cov + eps * eye\n",
    "\n",
    "    eigenvals, eigenvecs = torch.linalg.eigh(cov)\n",
    "\n",
    "    return eigenvals, eigenvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pointwise geometric features\n",
    "from segmentation.interactive_gc import scatter_eig\n",
    "from torch_geometric.nn import knn_graph, radius_graph\n",
    "from torch_scatter import scatter_mean\n",
    "import torch\n",
    "\n",
    "\n",
    "pos_t = torch.as_tensor(pos)\n",
    "# nb_i, ct_i = knn_graph(pos_t, k=16)\n",
    "edge_idx = knn_graph(pos_t, k=10)\n",
    "ct_i, nb_i = edge_idx\n",
    "\n",
    "rel_pos_nb = pos_t[nb_i] - pos_t[ct_i]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "evals, evecs = scatter_eigendecomposition(rel_pos_nb, ct_i, G = pos.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "eps = 1e-5\n",
    "l1, l2, l3 = evals[:,0], evals[:,1], evals[:,2]\n",
    "normals = evecs[:,:,0]\n",
    "verticality = normals[:,2].abs()\n",
    "linearity  = (l3 - l2) / (l3 + eps)\n",
    "planarity  = (l2 - l1) / (l3 + eps)\n",
    "scattering = l1 / (l3 + eps)\n",
    "sum_evals = l1 + l2 + l3 + eps\n",
    "omnivariance = (l1 * l2 * l3).pow(1/3)\n",
    "anisotropy = (l3 - l1) / (l3 + eps)\n",
    "p1 = l1 / sum_evals\n",
    "p2 = l2 / sum_evals\n",
    "p3 = l3 / sum_evals\n",
    "eigenentropy = -(p1 * torch.log(p1 + eps) + p2 * torch.log(p2 + eps) + p3 * torch.log(p3 + eps))\n",
    "change_curvature = l1 / sum_evals\n",
    "sphericity = change_curvature \n",
    "# surface_variation = l1 / (l1 + l2 + l3 + eps)\n",
    "# feature_flatness = l2 / (l3 + eps)\n",
    "\n",
    "feat = scattering.numpy()\n",
    "feat -= feat.min()\n",
    "feat /= feat.max()\n",
    "plotter2 = pv.Plotter(notebook=True)\n",
    "pcd_poly['feat'] = feat\n",
    "plotter2.add_mesh(pcd_poly,scalars='feat')\n",
    "plotter2.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_csr(edge_idx: torch.Tensor, num_nodes: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    sys.path.append(\"./segmentation/cutpursuit/pcd-prox-split/grid-graph/python\")\n",
    "    from grid_graph import edge_list_to_forward_star\n",
    "    indptr, indices, reidx = edge_list_to_forward_star(\n",
    "        num_nodes, edge_idx.T.contiguous().cpu().numpy()\n",
    "    )\n",
    "    return indptr.astype(np.uint32), indices.astype(np.uint32), reidx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorsys\n",
    "\n",
    "def index_to_color(indices):\n",
    "    \"\"\"\n",
    "    Map integer indices to vibrant RGB colors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    indices : int or array-like of ints\n",
    "        Input index or indices. Can be a scalar or an iterable of ints.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    colors : ndarray of shape (N, 3)\n",
    "        RGB colors in the range [0, 1], where N is the number of indices.\n",
    "    \"\"\"\n",
    "    # Ensure array of ints\n",
    "    indices = np.atleast_1d(np.array(indices, dtype=int).flatten())\n",
    "    \n",
    "    # Use the golden ratio conjugate to spread hues evenly\n",
    "    phi = 0.618033988749895\n",
    "    hues = (indices * phi) % 1.0\n",
    "    \n",
    "    # Vibrancy settings\n",
    "    s, v = 0.9, 0.95  # saturation and value\n",
    "    \n",
    "    # Convert each hue to RGB\n",
    "    colors = np.array([colorsys.hsv_to_rgb(h, s, v) for h in hues], dtype=float)\n",
    "    return colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build partition\n",
    "\n",
    "\n",
    "edge_idx = torch.cat([edge_idx, edge_idx.flip(0)], dim=1)\n",
    "src, tgt, _ = build_csr(edge_idx, pos.shape[0])\n",
    "\n",
    "cp_args_fine = dict(\n",
    "    regularization = 0.8,\n",
    "    spatial_weight = 100,\n",
    "    cutoff = 15,\n",
    "    cp_dif_tol =  1e-2,\n",
    "    cp_it_max = 20,\n",
    "    split_damp_ratio = 0.7\n",
    ")\n",
    "elevation = pos[:,2] - pos[:,2].min()\n",
    "feats_point = np.concat([\n",
    "    rgb,\n",
    "    intensity[:,None],\n",
    "    normals,\n",
    "    verticality[:,None],\n",
    "    linearity[:,None],\n",
    "    planarity[:,None],\n",
    "    scattering[:,None],\n",
    "    # sphericity[:,None],\n",
    "    elevation[:,None]\n",
    "],axis=1)\n",
    "\n",
    "\n",
    "feats_point -= feats_point.mean(axis=0)\n",
    "feats_point /= (np.std(feats_point,axis=0) + 1e-6)\n",
    "\n",
    "\n",
    "# fine cut-pursuit (get super_fine, graph_f)\n",
    "Df = pos.shape[1] + feats_point.shape[1]\n",
    "Xf = np.asfortranarray(np.concatenate([pos - pos.mean(0), feats_point], axis=1).T,\n",
    "                        dtype=np.float64)\n",
    "ew = np.ones_like(tgt, np.float64) * cp_args_fine.get('regularization', 0.1)\n",
    "vw = np.ones(pos.shape[0], np.float64)\n",
    "cw = np.ones(Df, np.float64)\n",
    "cw[:pos.shape[1]] *= cp_args_fine.get('spatial_weight', 10.0)\n",
    "sup_f, _, graph_f = cp_d0_dist(\n",
    "    Df, Xf, src, tgt,\n",
    "    edge_weights=ew, vert_weights=vw, coor_weights=cw,\n",
    "    min_comp_weight=cp_args_fine.get('cutoff',5),\n",
    "    cp_dif_tol=cp_args_fine.get('cp_dif_tol',1e-2),\n",
    "    cp_it_max=cp_args_fine.get('cp_it_max',10),\n",
    "    split_damp_ratio=cp_args_fine.get('split_damp_ratio',0.7),\n",
    "    verbose=False, max_num_threads=0,\n",
    "    balance_parallel_split=True,\n",
    "    compute_List=False, compute_Graph=True, compute_Time=False\n",
    ")\n",
    "\n",
    "sup_colors = index_to_color(sup_f)\n",
    "\n",
    "plotter3 = pv.Plotter(notebook=True)\n",
    "pcd_poly = pv.PolyData(pos)\n",
    "pcd_poly['RGB'] = sup_colors\n",
    "plotter3.add_mesh(pcd_poly,scalars='RGB',rgb=True)\n",
    "plotter3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_scatter import scatter_add\n",
    "\n",
    "container = torch.zeros((sup_f.max()+1,2),dtype=torch.int64)\n",
    "label_oh = torch.as_tensor(np.concatenate([(label_bin==0)[:,None], (label_bin==1)[:,None]],axis=1),dtype=torch.int64)\n",
    "\n",
    "container = scatter_add(label_oh,torch.as_tensor(sup_f,dtype=torch.int64),dim=0,dim_size=sup_f.max()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = container.float()\n",
    "totals = counts.sum(dim=1, keepdim=True)\n",
    "totals = torch.clamp(totals,min=1.0)\n",
    "probs = counts / totals\n",
    "eps=1e-8\n",
    "entropy = -(probs * torch.log(probs+eps)).sum(1)\n",
    "entropy_bits = entropy / np.log(2)\n",
    "sup_labels = torch.argmax(counts,dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sup_labels,bins=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate superpoint features\n",
    "rgb_pt = torch.tensor(rgb)\n",
    "intensity_pt = None\n",
    "normals_pt = normals\n",
    "verticality_pt = verticality[:,None]\n",
    "linearity_pt = linearity[:,None]\n",
    "planarity_pt = planarity[:,None]\n",
    "scattering_pt = scattering[:,None]\n",
    "sphericity_pt = sphericity[:,None]\n",
    "elevation_pt = torch.as_tensor(elevation[:,None]).float()\n",
    "\n",
    "# mean_pt_feat\n",
    "rgb_sup = scatter_mean(rgb_pt, torch.as_tensor(sup_f,dtype=torch.int64),dim=0) if rgb_pt is not None else None\n",
    "intensity_sup = scatter_mean(intensity_pt, torch.as_tensor(sup_f,dtype=torch.int64),dim=0) if intensity_pt is not None else None\n",
    "normals_sup = scatter_mean(normals_pt, torch.as_tensor(sup_f,dtype=torch.int64),dim=0) if normals_pt is not None else None\n",
    "verticality_sup = scatter_mean(verticality_pt, torch.as_tensor(sup_f,dtype=torch.int64),dim=0) if verticality_pt is not None else None\n",
    "linearity_sup = scatter_mean(linearity_pt, torch.as_tensor(sup_f,dtype=torch.int64),dim=0) if linearity_pt is not None else None\n",
    "planarity_sup = scatter_mean(planarity_pt, torch.as_tensor(sup_f,dtype=torch.int64),dim=0) if planarity_pt is not None else None\n",
    "scattering_sup = scatter_mean(scattering_pt, torch.as_tensor(sup_f,dtype=torch.int64),dim=0) if scattering_pt is not None else None\n",
    "sphericity_sup = scatter_mean(sphericity_pt, torch.as_tensor(sup_f,dtype=torch.int64),dim=0) if sphericity_pt is not None else None\n",
    "elevation_sup = scatter_mean(elevation_pt, torch.as_tensor(sup_f,dtype=torch.int64),dim=0) if elevation_pt is not None else None\n",
    "\n",
    "all_feats_sup = torch.concat([f for f in[rgb_sup,intensity_sup,normals_sup,verticality_sup,linearity_sup,planarity_sup,scattering_sup,sphericity_sup,elevation_sup] if f is not None],dim=1)\n",
    "\n",
    "feat = scattering_sup\n",
    "plotter4 = pv.Plotter(notebook=True)\n",
    "pcd_poly_feat = pv.PolyData(pos)\n",
    "pcd_poly_feat['sup_feat'] = feat[sup_f]\n",
    "plotter4.add_mesh(pcd_poly_feat, scalars='sup_feat')\n",
    "plotter4.show(jupyter_backend='trame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u,c = torch.unique(torch.as_tensor(sup_f,dtype=torch.int64), return_counts=True)\n",
    "plt.hist(c, bins=100)\n",
    "plt.title('Super point size distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_0 = feat[sup_labels==0]\n",
    "feat_1 = feat[sup_labels==1]\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(12,6))\n",
    "ax.hist(feat_0[:],bins=100, edgecolor='red',histtype='step',density=True,label='concrete')\n",
    "ax.hist(feat_1[:],bins=100, edgecolor='blue',histtype='step',density=True, label='brick')\n",
    "ax.legend()\n",
    "ax.set_title('Distribution of feature between classes')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.mixture import GaussianMixture\n",
    "# import random\n",
    "# print(all_feats_sup.shape)\n",
    "\n",
    "# k = 500\n",
    "# n_components = 10\n",
    "\n",
    "# all_feats_sup_trans = all_feats_sup - all_feats_sup.mean(dim=0)\n",
    "# all_feats_sup_trans /= all_feats_sup_trans.std(dim=0)\n",
    "\n",
    "# model_fg = GaussianMixture(n_components=n_components, covariance_type='full')\n",
    "# model_bg = GaussianMixture(n_components=n_components, covariance_type='full')\n",
    "\n",
    "\n",
    "# mask_0 = torch.tensor([v.item() for v in random.choices(torch.nonzero(sup_labels==0),k=k)])\n",
    "# mask_1 = torch.tensor([v.item() for v in random.choices(torch.nonzero(sup_labels==1),k=k)])\n",
    "\n",
    "\n",
    "# model_fg.fit(all_feats_sup[mask_0].numpy())\n",
    "# model_bg.fit(all_feats_sup[mask_1].numpy())\n",
    "\n",
    "\n",
    "# score_fg = model_fg.predict(all_feats_sup_trans.numpy())\n",
    "# score_bg = model_bg.predict(all_feats_sup_trans.numpy())\n",
    "\n",
    "# plotter5 = pv.Plotter(notebook=True)\n",
    "# pcd_poly_scores = pv.PolyData(pos)\n",
    "# pcd_poly_scores['s'] = np.zeros((pos.shape[0],3),dtype=np.float32)\n",
    "# pcd_poly_scores['s'] = np.concat([c[:,None] for c in [(score_bg/score_bg.max())[sup_f],np.zeros(pos.shape[0],dtype=float),np.zeros(pos.shape[0],dtype=float)]],axis=1)\n",
    "\n",
    "# pcd_poly_scores['s'][torch.isin(torch.as_tensor(sup_f).long(),mask_0)] = [0,0,1]\n",
    "# pcd_poly_scores['s'][torch.isin(torch.as_tensor(sup_f).long(),mask_1)] = [0,1,0]\n",
    "\n",
    "# plotter5.add_mesh(pcd_poly_scores,scalars='s',rgb=True)\n",
    "# plotter5.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((score_bg > 0.5) == sup_labels).sum()/sup_labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from typing import List\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,channels:List[int],dropout:List[float]|float|None = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp = nn.Sequential(*[nn.Linear(channels[i], channels[i+1]) for i in range(len(channels) - 1)])\n",
    "        if isinstance(dropout, float):\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "        elif isinstance(dropout, list):\n",
    "            assert len(dropout) == len(channels)-1\n",
    "            self.dropout = nn.ModuleList([nn.Dropout(p) for p in dropout])\n",
    "        else:\n",
    "            self.dropout = None\n",
    "    def forward(self, x):\n",
    "        for i, m in enumerate(self.mlp):\n",
    "            x = m(x)\n",
    "            if i < len(self.mlp) -1:\n",
    "                x = nn.functional.relu(x)\n",
    "            if self.dropout is not None:\n",
    "                if isinstance(self.dropout, nn.ModuleList):\n",
    "                    x = self.dropout[i](x)\n",
    "                else:\n",
    "                    x = self.dropout(x)\n",
    "        x = torch.nn.functional.log_softmax(x,dim=1)\n",
    "        return x\n",
    "    def fit(self,\n",
    "            x:torch.Tensor, \n",
    "            y:torch.Tensor,\n",
    "            criterion = torch.nn.CrossEntropyLoss(),\n",
    "            epochs:int=100,\n",
    "            device='cuda',\n",
    "            lr=0.01,\n",
    "            weight_decay:float = 1e-4,\n",
    "            optim:torch.optim.Optimizer|None=None,\n",
    "            ):\n",
    "        self.train()\n",
    "        if optim is None:\n",
    "            optim = torch.optim.Adam(\n",
    "                self.parameters(),\n",
    "                lr=lr,\n",
    "                weight_decay=weight_decay\n",
    "                )\n",
    "        x = x.to(device=device)\n",
    "        y = y.to(device=device)\n",
    "        self.to(device=device)\n",
    "        loss = torch.tensor(0)\n",
    "        for epoch in range(1,epochs+1):\n",
    "            optim.zero_grad()\n",
    "            out = self(x)\n",
    "            loss = criterion(out,y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        print(f\"DEBUG fit MLP with loss={loss.detach().cpu().item():.3f}\")\n",
    "    def predict(self, x):\n",
    "        self.eval()\n",
    "        return self(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "all_feats_sup_trans = all_feats_sup - all_feats_sup.mean(dim=0)\n",
    "all_feats_sup_trans /= all_feats_sup_trans.std(dim=0)\n",
    "sup_centroids = scatter_mean(torch.as_tensor(pos), torch.as_tensor(sup_f,dtype=torch.int64),dim=0)\n",
    "\n",
    "\n",
    "# n_dims = all_feats_sup_trans.shape[1]\n",
    "# models = {\n",
    "#   \"Logistic regression\": MLP([n_dims,2]),  \n",
    "# #   \"1 hidden layer (64)\": MLP([n_dims,64,2]),\n",
    "# #   \"1 hidden layer (64) + dropout(0.2)\": MLP([n_dims,64,2],dropout=0.2)\n",
    "# } \n",
    "\n",
    "\n",
    "# ks = [1,2,3,4,5,10,15,20]\n",
    "# ks = [15]\n",
    "# num_attempts = 1\n",
    "# fig,ax = plt.subplots(1,1,figsize=(12,6))\n",
    "# for mi, (model_name, model) in enumerate(models.items()):\n",
    "#     all_accs = []\n",
    "#     for attempt_i in range(num_attempts):\n",
    "#         accs = []\n",
    "#         for k in ks:\n",
    "            \n",
    "\n",
    "#             mask_fg = torch.tensor([v.item() for v in random.choices(torch.nonzero(sup_labels==0),k=k)])\n",
    "#             mask_bg = torch.tensor([v.item() for v in random.choices(torch.nonzero(sup_labels==1),k=k)])\n",
    "\n",
    "#             mask_train = torch.zeros(sup_labels.shape[0],dtype=torch.int64).bool()\n",
    "#             mask_train[mask_fg] = True\n",
    "#             mask_train[mask_bg] = True\n",
    "\n",
    "\n",
    "\n",
    "#             train_feats = all_feats_sup_trans[mask_train]\n",
    "#             train_y = sup_labels[mask_train]\n",
    "\n",
    "#             model.fit(train_feats, train_y,epochs=1000)\n",
    "\n",
    "#             pred = model.predict(all_feats_sup_trans.to(device='cuda')).detach().cpu().numpy()\n",
    "#             pred = np.argmax(pred, axis=1)\n",
    "#             acc = (sup_labels == pred).sum() / pred.shape[0]\n",
    "#             accs.append(acc)\n",
    "#         all_accs.append(accs)\n",
    "#     all_accs = np.array(all_accs)\n",
    "#     accs = np.mean(all_accs, axis=0)\n",
    "#     accs_std = np.std(all_accs,axis=0)\n",
    "#     color=index_to_color(mi)\n",
    "#     ks_s = np.array(ks)\n",
    "#     ax.errorbar(ks_s,accs,yerr= accs_std,fmt='o', capsize=5, label=model_name, color = color,linestyle='none')\n",
    "#     ax.plot(ks_s,accs,color=color)\n",
    "#     ax.fill_between(ks_s,accs-accs_std,accs+accs_std, color=color, alpha=0.2,)\n",
    "\n",
    "# ax.set_xticks(ks)\n",
    "# ax.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = torch.zeros(sup_f.shape[0], dtype = torch.int64)\n",
    "# y[mask_fg]=1\n",
    "\n",
    "\n",
    "# pred_viz = model.predict(all_feats_sup_trans.to(device='cuda')).detach().cpu().numpy()[:,1]\n",
    "# pred_viz -= pred.min()\n",
    "# pred_viz /= pred.max()\n",
    "# plotter6 = pv.Plotter(notebook=True)\n",
    "# pcd_poly_scores = pv.PolyData(pos)\n",
    "\n",
    "# pcd_poly_scores['s'] = pred_viz[sup_f]\n",
    "\n",
    "# # pcd_poly_scores['s'][torch.isin(torch.as_tensor(sup_f).long(),mask_0)] = [0,0,1]\n",
    "# # pcd_poly_scores['s'][torch.isin(torch.as_tensor(sup_f).long(),mask_1)] = [0,1,0]\n",
    "\n",
    "# plotter6.add_mesh(pcd_poly_scores,scalars='s',rgb=False)\n",
    "# plotter6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_mrf(node_feats, cost_source, cost_sink, first_edg, adj_verts):\n",
    "    d2 = []\n",
    "    for u in range(V):\n",
    "        for k in range(first_edg[u], first_edg[u+1]):\n",
    "            v = adj_verts[k]\n",
    "            d2.append(np.sum((node_feats[u] - node_feats[v])**2))\n",
    "    d2 = np.array(d2)\n",
    "    m = np.median(d2)\n",
    "\n",
    "    beta = np.log(2)/m\n",
    "    delta = np.abs(cost_source - cost_sink)\n",
    "    lmbda = np.median(delta)\n",
    "\n",
    "    return beta, lmbda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import maxflow\n",
    "def solve_mincut(node_feats, cost_source, cost_sink, first_edg, adj_verts, lmbda, beta):\n",
    "    \n",
    "    V,F = node_feats.shape\n",
    "    g = maxflow.Graph[float](V, adj_verts.size)\n",
    "    g.add_nodes(V)\n",
    "    # t-links\n",
    "    for u in range(V):\n",
    "        g.add_tedge(u, cost_source[u], cost_sink[u])\n",
    "\n",
    "    # pairwise links\n",
    "    for u in range(V):\n",
    "        start, end = first_edg[u], first_edg[u+1]\n",
    "        u_feat = node_feats[u]\n",
    "        for idx in range(start,end):\n",
    "            v = adj_verts[idx]\n",
    "            v_feat = node_feats[v]\n",
    "            diff = u_feat - v_feat\n",
    "            w = lmbda * np.exp(-beta * (diff@diff))\n",
    "            g.add_edge(u,v,w,w)\n",
    "    # solve\n",
    "    flow = g.maxflow()\n",
    "    print(f\"DEBUG solved mincut with {flow=}\")\n",
    "\n",
    "\n",
    "    # y = torch.zeros(sup_f.shape[0], dtype = torch.int64)\n",
    "    # y[mask_fg]=1\n",
    "\n",
    "    labels = np.array([g.get_segment(u) for u in range(V)])\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# constant picks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# ### CONFIG\n",
    "config_k = 20 # picks per class\n",
    "\n",
    "# initial mask (simulated picks)\n",
    "mask_fg = torch.tensor([v.item() for v in random.choices(torch.nonzero(sup_labels==0),k=config_k)])\n",
    "mask_bg = torch.tensor([v.item() for v in random.choices(torch.nonzero(sup_labels==1),k=config_k)])\n",
    "\n",
    "mask_train = torch.zeros(sup_labels.shape[0],dtype=torch.int64).bool()\n",
    "mask_train[mask_fg] = True\n",
    "mask_train[mask_bg] = True\n",
    "\n",
    "sup_centroids_fg = sup_centroids[mask_fg]\n",
    "sup_centroids_bg = sup_centroids[mask_bg]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.nn import fps\n",
    "\n",
    "\n",
    "# sup_centroids_fg = fps(sup_centroids[sup_labels==0],ratio=(config_k / (sup_labels==0).sum()).item())\n",
    "# sup_centroids_bg = fps(sup_centroids[sup_labels==1],ratio=(config_k / (sup_labels==1).sum()).item())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min-cut solution\n",
    "import maxflow\n",
    "import numpy as np\n",
    "\n",
    "### CONFIG\n",
    "config_epochs = 1000\n",
    "\n",
    "\n",
    "\n",
    "# inputs\n",
    "V,F = all_feats_sup_trans.shape\n",
    "node_feats = all_feats_sup_trans.numpy()\n",
    "node_feats_t = torch.as_tensor(node_feats)\n",
    "model = MLP([node_feats.shape[1],64,2],dropout=0.2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_feats = all_feats_sup_trans[mask_train]\n",
    "train_y = sup_labels[mask_train]\n",
    "\n",
    "# grab graph info\n",
    "first_edg = graph_f[0] # [V]\n",
    "adj_verts = graph_f[1] # [E]\n",
    "\n",
    "# solve\n",
    "# iteration 0\n",
    "model.fit(train_feats, train_y,epochs=config_epochs)\n",
    "preds = model(node_feats_t.to(device='cuda')).detach().cpu().numpy()\n",
    "print(f\"Unary Accuracy: {((sup_labels.numpy() == np.argmax(preds, axis=1)).sum()/preds.shape[0]).item()*100:.3f}%\")\n",
    "cost_source     = - preds[:,1]\n",
    "cost_sink       = - preds[:,0]\n",
    "\n",
    "\n",
    "beta,lmbda = calibrate_mrf(node_feats, cost_source, cost_sink, first_edg, adj_verts)\n",
    "labels = solve_mincut(node_feats, cost_source, cost_sink, first_edg, adj_verts, lmbda, beta)\n",
    "print(f\"Accuracy: {((sup_labels.numpy() == labels.astype(bool)).sum()/labels.shape[0]).item()*100:.3f}%\")\n",
    "\n",
    "# iteration 1\n",
    "labels = torch.as_tensor(labels,dtype=torch.int64)\n",
    "node_feats_t = all_feats_sup_trans\n",
    "model.fit(node_feats_t, labels, epochs=config_epochs)\n",
    "preds = model(node_feats_t.to(device='cuda')).detach().cpu().numpy()\n",
    "cost_source     = - preds[:,1]\n",
    "cost_sink       = - preds[:,0]\n",
    "labels = solve_mincut(node_feats, cost_source, cost_sink, first_edg, adj_verts, lmbda, beta)\n",
    "print(f\"Accuracy: {((sup_labels.numpy() == labels.astype(bool)).sum()/labels.shape[0]).item()*100:.3f}%\")\n",
    "\n",
    "# iteration 2\n",
    "labels = torch.as_tensor(labels,dtype=torch.int64)\n",
    "node_feats_t = all_feats_sup_trans\n",
    "model.fit(node_feats_t, labels, epochs=config_epochs)\n",
    "preds = model(node_feats_t.to(device='cuda')).detach().cpu().numpy()\n",
    "cost_source     = - preds[:,1]\n",
    "cost_sink       = - preds[:,0]\n",
    "labels = solve_mincut(node_feats, cost_source, cost_sink, first_edg, adj_verts, lmbda, beta)\n",
    "print(f\"Accuracy: {((sup_labels.numpy() == labels.astype(bool)).sum()/labels.shape[0]).item()*100:.3f}%\")\n",
    "\n",
    "\n",
    "# iteration 3\n",
    "labels = torch.as_tensor(labels,dtype=torch.int64)\n",
    "node_feats_t = all_feats_sup_trans\n",
    "model.fit(node_feats_t, labels, epochs=config_epochs)\n",
    "preds = model(node_feats_t.to(device='cuda')).detach().cpu().numpy()\n",
    "cost_source     = - preds[:,1]\n",
    "cost_sink       = - preds[:,0]\n",
    "labels = solve_mincut(node_feats, cost_source, cost_sink, first_edg, adj_verts, lmbda, beta)\n",
    "print(f\"Accuracy: {((sup_labels.numpy() == labels.astype(bool)).sum()/labels.shape[0]).item()*100:.3f}%\")\n",
    "\n",
    "\n",
    "# iteration 4\n",
    "labels = torch.as_tensor(labels,dtype=torch.int64)\n",
    "node_feats_t = all_feats_sup_trans\n",
    "model.fit(node_feats_t, labels, epochs=config_epochs)\n",
    "preds = model(node_feats_t.to(device='cuda')).detach().cpu().numpy()\n",
    "cost_source     = - preds[:,1]\n",
    "cost_sink       = - preds[:,0]\n",
    "labels = solve_mincut(node_feats, cost_source, cost_sink, first_edg, adj_verts, lmbda, beta)\n",
    "print(f\"Accuracy: {((sup_labels.numpy() == labels.astype(bool)).sum()/labels.shape[0]).item()*100:.3f}%\")\n",
    "\n",
    "# iteration 4\n",
    "labels = torch.as_tensor(labels,dtype=torch.int64)\n",
    "node_feats_t = all_feats_sup_trans\n",
    "model.fit(node_feats_t, labels, epochs=config_epochs)\n",
    "preds = model(node_feats_t.to(device='cuda')).detach().cpu().numpy()\n",
    "cost_source     = - preds[:,1]\n",
    "cost_sink       = - preds[:,0]\n",
    "labels = solve_mincut(node_feats, cost_source, cost_sink, first_edg, adj_verts, lmbda, beta)\n",
    "print(f\"Accuracy: {((sup_labels.numpy() == labels.astype(bool)).sum()/labels.shape[0]).item()*100:.3f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plotter7 = pv.Plotter(notebook=True)\n",
    "pcd_poly_mincut = pv.PolyData(pos)\n",
    "\n",
    "pcd_poly_mincut['s'] = labels[sup_f]\n",
    "\n",
    "\n",
    "# pcd_poly_scores['s'][torch.isin(torch.as_tensor(sup_f).long(),mask_0)] = [0,0,1]\n",
    "# pcd_poly_scores['s'][torch.isin(torch.as_tensor(sup_f).long(),mask_1)] = [0,1,0]\n",
    "\n",
    "plotter7.add_mesh(pcd_poly_mincut,scalars='s',rgb=False,cmap=['#ae8484','#c7e881'],categories=True)\n",
    "\n",
    "for centroid in sup_centroids_fg:\n",
    "    sp = pv.Sphere(0.2,centroid.numpy())\n",
    "    plotter7.add_mesh(sp, color='f01c1c')\n",
    "\n",
    "for centroid in sup_centroids_bg:\n",
    "    sp = pv.Sphere(0.2,centroid.numpy())\n",
    "    plotter7.add_mesh(sp, color='#a8f209')\n",
    "\n",
    "plotter7.show()\n",
    "reset_camera_standard(plotter7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min-cut solution\n",
    "import maxflow\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# informed picking\n",
    "# initial mask\n",
    "\n",
    "### CONFIG\n",
    "config_init_k = 3 # picks per class\n",
    "config_k_per_it = 1\n",
    "num_iterations = 30\n",
    "\n",
    "\n",
    "# initial mask (simulated picks)\n",
    "mask_fg = torch.tensor(random.sample(torch.nonzero(sup_labels==0).tolist(),k=config_init_k))\n",
    "mask_bg = torch.tensor(random.sample(torch.nonzero(sup_labels==1).tolist(),k=config_init_k))\n",
    "\n",
    "\n",
    "mask_train = torch.zeros(sup_labels.shape[0],dtype=torch.int64).bool()\n",
    "mask_train[mask_fg] = True\n",
    "mask_train[mask_bg] = True\n",
    "\n",
    "\n",
    "config_epochs = 1000\n",
    "\n",
    "\n",
    "\n",
    "# inputs\n",
    "V,F = all_feats_sup_trans.shape\n",
    "node_feats = all_feats_sup_trans.numpy()\n",
    "node_feats_t = torch.as_tensor(node_feats)\n",
    "model = MLP([node_feats.shape[1],64,2],dropout=0.2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_feats = all_feats_sup_trans[mask_train]\n",
    "train_y = sup_labels[mask_train]\n",
    "\n",
    "# grab graph info\n",
    "first_edg = graph_f[0] # [V]\n",
    "adj_verts = graph_f[1] # [E]\n",
    "\n",
    "# solve\n",
    "# iteration 0\n",
    "\n",
    "\n",
    "\n",
    "def fit_and_solve(model,train_feats, train_y,num_epochs,sup_labels,node_feats_t,first_edg,adj_verts):\n",
    "\n",
    "    model.fit(train_feats, train_y,epochs=num_epochs)\n",
    "    preds = model(node_feats_t.to(device='cuda')).detach().cpu().numpy()\n",
    "    print(f\"Unary Accuracy: {((sup_labels.numpy() == np.argmax(preds, axis=1)).sum()/preds.shape[0]).item()*100:.3f}%\")\n",
    "    cost_source     = - preds[:,1]\n",
    "    cost_sink       = - preds[:,0]\n",
    "\n",
    "\n",
    "    beta,lmbda = calibrate_mrf(node_feats, cost_source, cost_sink, first_edg, adj_verts)\n",
    "    mc_preds = solve_mincut(node_feats, cost_source, cost_sink, first_edg, adj_verts, lmbda, beta)\n",
    "    print(f\"Accuracy: {((sup_labels.numpy() == mc_preds.astype(bool)).sum()/mc_preds.shape[0]).item()*100:.3f}%\")\n",
    "    \n",
    "\n",
    "    return mc_preds, mc_preds == sup_labels.numpy()\n",
    "\n",
    "# init solve\n",
    "labels, correct = fit_and_solve(model,train_feats, train_y,config_epochs,sup_labels,node_feats_t,first_edg,adj_verts)\n",
    "acc_best = 0\n",
    "labels_best = None\n",
    "for it in range(num_iterations):\n",
    "    train_feats = all_feats_sup_trans[mask_train]\n",
    "    train_y = sup_labels[mask_train]\n",
    "    # update train mask from wrong classifications\n",
    "\n",
    "\n",
    "    wrong_sup_idx = np.nonzero(~correct)[0]\n",
    "    new_samples_fg = random.sample(wrong_sup_idx[sup_labels[wrong_sup_idx]==0].tolist(), k=config_k_per_it)\n",
    "    new_samples_bg = random.sample(wrong_sup_idx[sup_labels[wrong_sup_idx]==1].tolist(), k=config_k_per_it)\n",
    "    mask_train[new_samples_bg] = True\n",
    "    mask_train[new_samples_fg] = True\n",
    "    labels, correct = fit_and_solve(model,train_feats, train_y,config_epochs,sup_labels,node_feats_t,first_edg,adj_verts)\n",
    "    acc_new = correct.mean()\n",
    "    if acc_new > acc_best:\n",
    "        acc_best = acc_new\n",
    "        labels_best = labels\n",
    "        print(f'>>>>>>> new best acc: {acc_best} <<<<<<<<<<<')\n",
    "plotter8 = pv.Plotter(notebook=True)\n",
    "pcd_poly_mincut = pv.PolyData(pos)\n",
    "\n",
    "pcd_poly_mincut['s'] = labels[sup_f]\n",
    "\n",
    "\n",
    "# pcd_poly_scores['s'][torch.isin(torch.as_tensor(sup_f).long(),mask_0)] = [0,0,1]\n",
    "# pcd_poly_scores['s'][torch.isin(torch.as_tensor(sup_f).long(),mask_1)] = [0,1,0]\n",
    "\n",
    "plotter8.add_mesh(pcd_poly_mincut,scalars='s',rgb=False,cmap=['#ae8484','#c7e881'],categories=True)\n",
    "# update centroids to reflect mask\n",
    "sup_centroids_fg = sup_centroids[torch.logical_and(mask_train, sup_labels==0)]\n",
    "sup_centroids_bg = sup_centroids[torch.logical_and(mask_train, sup_labels==1)]\n",
    "\n",
    "for centroid in sup_centroids_fg:\n",
    "    sp = pv.Sphere(0.2,centroid.numpy())\n",
    "    plotter8.add_mesh(sp, color='f01c1c')\n",
    "\n",
    "for centroid in sup_centroids_bg:\n",
    "    sp = pv.Sphere(0.2,centroid.numpy())\n",
    "    plotter8.add_mesh(sp, color='#a8f209')\n",
    "\n",
    "plotter8.show()\n",
    "reset_camera_standard(plotter8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embeddings models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.graph import SimpleModel, SPConvModel, PointNetEncoder, forwardstar_to_idx, SPConv\n",
    "from utils.pointcloud import compute_geometric_feats\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# config_model_fp = 'experiments/spconv_contrastive_L0-1.pth'\n",
    "# config_model_fp = 'experiments/spconv_contrastive_L0-01.pth'\n",
    "# config_model_fp = 'experiments/spconv_contrastive_L1-00.pth'\n",
    "config_model_fp = 'experiments/simple_model.pth'\n",
    "config_device = 'cuda'\n",
    "\n",
    "\n",
    "model = torch.load(config_model_fp, weights_only=False)\n",
    "model.eval().to(device=config_device)\n",
    "\n",
    "# convert and direct edges\n",
    "edge_idx = forwardstar_to_idx(first_edg, adj_verts)\n",
    "edge_idx = torch.tensor(np.concat([edge_idx,edge_idx[[1,0],:]],axis=1),dtype=torch.int64)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gfeats_preembed = compute_geometric_feats(torch.as_tensor(pos).to(device=config_device), torch.as_tensor(edge_idx,dtype=torch.int64).to(device=config_device), feat_names=[\n",
    "    'normals', \n",
    "    'verticality', \n",
    "    'linearity', \n",
    "    'planarity', \n",
    "    'scattering', \n",
    "])      \n",
    "\n",
    "pfeats_preembed = np.concatenate([\n",
    "    rgb,\n",
    "    intensity[:,None],\n",
    "    gfeats_preembed['normals'].cpu().numpy(), \n",
    "    gfeats_preembed['verticality'][:,None].cpu().numpy(), \n",
    "    gfeats_preembed['linearity'][:,None].cpu().numpy(), \n",
    "    gfeats_preembed['planarity'][:,None].cpu().numpy(), \n",
    "    gfeats_preembed['scattering'][:,None].cpu().numpy(), \n",
    "    pos[:,2][:,None],\n",
    "    \n",
    "],axis=1)\n",
    "\n",
    "pfeats_preembed -= pfeats_preembed.mean(axis=0)\n",
    "pfeats_preembed /= (np.std(pfeats_preembed, axis=0) + 1e-6)\n",
    "\n",
    "\n",
    "# forward pass embedding model\n",
    "if isinstance(model, SimpleModel):\n",
    "    z, _ = model(\n",
    "        torch.as_tensor(pos,dtype=torch.float32).to(device=config_device),\n",
    "        torch.as_tensor(pfeats_preembed,dtype=torch.float32).to(device=config_device),\n",
    "        torch.as_tensor(sup_f,dtype=torch.int64).to(device=config_device),\n",
    "        )\n",
    "elif isinstance(model, SPConvModel):\n",
    "    z, _ = model(\n",
    "        torch.as_tensor(pos,dtype=torch.float32).to(device=config_device),\n",
    "        torch.as_tensor(pfeats_preembed,dtype=torch.float32).to(device=config_device),\n",
    "        torch.as_tensor(sup_f,dtype=torch.int64).to(device=config_device),\n",
    "        torch.as_tensor(edge_idx,dtype=torch.int64).to(device=config_device),\n",
    "        )\n",
    "else:\n",
    "    raise NotImplementedError(f\"No procedure implemented for {model.__class__.__name__}\")\n",
    "\n",
    "z = z.detach().cpu()\n",
    "sup_feat = scatter_mean(torch.as_tensor(pfeats_preembed), torch.as_tensor(sup_f,dtype=torch.int64),dim=0)\n",
    "total_feat = torch.concat([sup_feat, z],dim=1)\n",
    "total_feat -= total_feat.mean(dim=0)\n",
    "total_feat /= (torch.std(total_feat) + 1e-6)\n",
    "\n",
    "# z = z.detach().cpu()\n",
    "# z -= z.mean()\n",
    "# z  /= torch.std(z) + 1e-6\n",
    "\n",
    "############################################\n",
    "# interactive seg on calculated embeddings #\n",
    "############################################\n",
    "\n",
    "# min-cut solution\n",
    "import maxflow\n",
    "import numpy as np\n",
    "\n",
    "# informed picking\n",
    "# initial mask\n",
    "\n",
    "### CONFIG\n",
    "config_init_k = 3 # picks per class\n",
    "config_k_per_it = 1\n",
    "num_iterations = 30\n",
    "config_epochs = 1000\n",
    "\n",
    "# initial mask (simulated picks)\n",
    "mask_fg = torch.tensor(random.sample(torch.nonzero(sup_labels==0).tolist(),k=config_init_k))\n",
    "mask_bg = torch.tensor(random.sample(torch.nonzero(sup_labels==1).tolist(),k=config_init_k))\n",
    "\n",
    "\n",
    "mask_train = torch.zeros(sup_labels.shape[0],dtype=torch.int64).bool()\n",
    "mask_train[mask_fg] = True\n",
    "mask_train[mask_bg] = True\n",
    "\n",
    "# inputs\n",
    "V,F = total_feat.shape\n",
    "node_feats = total_feat.numpy()\n",
    "node_feats_t = torch.as_tensor(node_feats)\n",
    "model = MLP([node_feats.shape[1],64,2],dropout=0.5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_feats = total_feat[mask_train]\n",
    "train_y = sup_labels[mask_train]\n",
    "\n",
    "# grab graph info\n",
    "first_edg = graph_f[0] # [V]\n",
    "adj_verts = graph_f[1] # [E]\n",
    "\n",
    "# solve\n",
    "# iteration 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# init solve\n",
    "labels, correct = fit_and_solve(model,train_feats, train_y,config_epochs,sup_labels,node_feats_t,first_edg,adj_verts)\n",
    "\n",
    "for it in range(num_iterations):\n",
    "    train_feats = total_feat[mask_train]\n",
    "    train_y = sup_labels[mask_train]\n",
    "    # update train mask from wrong classifications\n",
    "\n",
    "\n",
    "    wrong_sup_idx = np.nonzero(~correct)[0]\n",
    "    new_samples_fg = random.sample(wrong_sup_idx[sup_labels[wrong_sup_idx]==0].tolist(), k=config_k_per_it)\n",
    "    new_samples_bg = random.sample(wrong_sup_idx[sup_labels[wrong_sup_idx]==1].tolist(), k=config_k_per_it)\n",
    "    mask_train[new_samples_bg] = True\n",
    "    mask_train[new_samples_fg] = True\n",
    "    labels, correct = fit_and_solve(model,train_feats, train_y,config_epochs,sup_labels,node_feats_t,first_edg,adj_verts)\n",
    "    \n",
    "plotter9 = pv.Plotter(notebook=True)\n",
    "pcd_poly_mincut_z = pv.PolyData(pos)\n",
    "\n",
    "pcd_poly_mincut_z['s'] = labels[sup_f]\n",
    "\n",
    "\n",
    "# pcd_poly_scores['s'][torch.isin(torch.as_tensor(sup_f).long(),mask_0)] = [0,0,1]\n",
    "# pcd_poly_scores['s'][torch.isin(torch.as_tensor(sup_f).long(),mask_1)] = [0,1,0]\n",
    "\n",
    "plotter9.add_mesh(pcd_poly_mincut_z,scalars='s',rgb=False,cmap=['#ae8484','#c7e881'],categories=True)\n",
    "# update centroids to reflect mask\n",
    "sup_centroids_fg = sup_centroids[torch.logical_and(mask_train, sup_labels==0)]\n",
    "sup_centroids_bg = sup_centroids[torch.logical_and(mask_train, sup_labels==1)]\n",
    "\n",
    "for centroid in sup_centroids_fg:\n",
    "    sp = pv.Sphere(0.2,centroid.numpy())\n",
    "    plotter9.add_mesh(sp, color='f01c1c')\n",
    "\n",
    "for centroid in sup_centroids_bg:\n",
    "    sp = pv.Sphere(0.2,centroid.numpy())\n",
    "    plotter9.add_mesh(sp, color='#a8f209')\n",
    "\n",
    "plotter9.show()\n",
    "reset_camera_standard(plotter9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple HC features + reggrow (Composite model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation.reggrow_params import *\n",
    "from segmentation.reggrow.build.lib import reggrow as rg\n",
    "\n",
    "rg_pcd = rg.PointCloud(pos, None)\n",
    "rg_params = ReggrowParams()\n",
    "rg_params.build_params.epsilon=0.09\n",
    "graph = rg.region_graph(rg_pcd,1,1,1, **rg_params.build_params.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_reg_ids = np.unique(graph['region_assignment'])\n",
    "reg_ids = graph['region_assignment']\n",
    "\n",
    "if -1 in u_reg_ids:\n",
    "    u_reg_ids += 1\n",
    "    reg_ids += 1\n",
    "    \n",
    "\n",
    "# calculate geometry on each region\n",
    "evals, evecs = scatter_eigendecomposition(torch.as_tensor(pos, dtype=torch.float32), torch.as_tensor(reg_ids, dtype=torch.int64),reg_ids.max()+1)\n",
    "\n",
    "# different feature flavors\n",
    "eps = 1e-5\n",
    "l1, l2, l3 = evals[:,0], evals[:,1], evals[:,2]\n",
    "normals = evecs[:,:,0]\n",
    "verticality = normals[:,2].abs()\n",
    "linearity  = (l3 - l2) / (l3 + eps)\n",
    "planarity  = (l2 - l1) / (l3 + eps)\n",
    "scattering = l1 / (l3 + eps)\n",
    "sum_evals = l1 + l2 + l3 + eps\n",
    "sphericity = l1 / sum_evals    \n",
    "\n",
    "reg_geom_feats = torch.cat([\n",
    "    normals, verticality[:,None], linearity[:,None], planarity[:,None], scattering[:,None], sum_evals[:,None], sphericity[:,None],\n",
    "],dim=1)\n",
    "# expand to point cloud size\n",
    "reg_geom_feats = reg_geom_feats[reg_ids]\n",
    "# average for every superpoint\n",
    "reg_geom_feats = scatter_mean(reg_geom_feats, torch.as_tensor(sup_f, dtype=torch.int64),dim=0)\n",
    "reg_geom_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ids_oh = torch.as_tensor(np.eye(reg_ids.max()+1)[reg_ids])\n",
    "reg_ids_oh = scatter_mean(reg_ids_oh, torch.as_tensor(sup_f, dtype=torch.int64),dim=0)\n",
    "reg_ids_oh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate geometry of each superpoint\n",
    "_, sup_f_inv = np.unique(sup_f, return_inverse=True)\n",
    "evals, evecs = scatter_eigendecomposition(torch.as_tensor(pos, dtype=torch.float32), torch.as_tensor(sup_f, dtype=torch.int64),sup_f_inv.max()+1)\n",
    "\n",
    "# different feature flavors\n",
    "eps = 1e-5\n",
    "l1, l2, l3 = evals[:,0], evals[:,1], evals[:,2]\n",
    "normals = evecs[:,:,0]\n",
    "verticality = normals[:,2].abs()\n",
    "linearity  = (l3 - l2) / (l3 + eps)\n",
    "planarity  = (l2 - l1) / (l3 + eps)\n",
    "scattering = l1 / (l3 + eps)\n",
    "sum_evals = l1 + l2 + l3 + eps\n",
    "sphericity = l1 / sum_evals    \n",
    "\n",
    "sup_geom_feats = torch.cat([\n",
    "    normals, verticality[:,None], linearity[:,None], planarity[:,None], scattering[:,None], sum_evals[:,None], sphericity[:,None],\n",
    "],dim=1)\n",
    "# expand to point cloud size\n",
    "sup_geom_feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_reg_lut = MLP([reg_ids_oh.shape[1],32])\n",
    "mlp_reg_geom = MLP([9,32], dropout=0.2)\n",
    "mlp_sup_geom = MLP([9,32], dropout=0.2)\n",
    "mlp_pts_geom = MLP([12,32], dropout=0.2)\n",
    "\n",
    "class CompositeModel(nn.Module):\n",
    "    def __init__(self,\n",
    "        mlp_reg_lut:nn.Module|None = None,\n",
    "        mlp_reg_geom:nn.Module|None = None,\n",
    "        mlp_sup_geom:nn.Module|None = None,\n",
    "        mlp_pts_geom:nn.Module|None = None,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.mlp_reg_lut    = mlp_reg_lut\n",
    "        self.mlp_reg_geom   = mlp_reg_geom\n",
    "        self.mlp_sup_geom   = mlp_sup_geom\n",
    "        self.mlp_pts_geom   = mlp_pts_geom\n",
    "        \n",
    "        self.mlp_unary = MLP([32,2], dropout=2)\n",
    "        \n",
    "    def forward(self,x:Dict[str, torch.Tensor]\n",
    "        # reg_lut:torch.Tensor,\n",
    "        # reg_geom:torch.Tensor,\n",
    "        # sup_geom:torch.Tensor,\n",
    "        # pts_geom:torch.Tensor,\n",
    "        ):\n",
    "        \n",
    "        out_mlp_reg_lut    = self.mlp_reg_lut(x['reg_lut'])      if self.mlp_reg_lut    is not None else None\n",
    "        out_mlp_reg_geom   = self.mlp_reg_geom(x['reg_geom'])    if self.mlp_reg_geom   is not None else None\n",
    "        out_mlp_sup_geom   = self.mlp_sup_geom(x['sup_geom'])    if self.mlp_sup_geom   is not None else None\n",
    "        out_mlp_pts_geom   = self.mlp_pts_geom(x['pts_geom'])    if self.mlp_pts_geom   is not None else None\n",
    "\n",
    "        embeddings = [e for e in [out_mlp_reg_lut, out_mlp_reg_geom, out_mlp_sup_geom, out_mlp_pts_geom] if e is not None]\n",
    "        embeddings = torch.stack(embeddings,dim=0).mean(dim=0)\n",
    "        \n",
    "        unary = self.mlp_unary(embeddings)\n",
    "        return unary\n",
    "    \n",
    "    def fit(self,\n",
    "            x:Dict[str, torch.Tensor], \n",
    "            y:torch.Tensor,\n",
    "            criterion = torch.nn.CrossEntropyLoss(),\n",
    "            epochs:int=100,\n",
    "            device='cuda',\n",
    "            lr=0.01,\n",
    "            weight_decay:float = 1e-4,\n",
    "            optim:torch.optim.Optimizer|None=None,\n",
    "            ):\n",
    "        self.train()\n",
    "        if optim is None:\n",
    "            optim = torch.optim.Adam(\n",
    "                self.parameters(),\n",
    "                lr=lr,\n",
    "                weight_decay=weight_decay\n",
    "                )\n",
    "\n",
    "        y = y.to(device=device)\n",
    "        self.to(device=device)\n",
    "        loss = torch.tensor(0)\n",
    "        for epoch in range(1,epochs+1):\n",
    "            optim.zero_grad()\n",
    "            out = self(x)\n",
    "            loss = criterion(out,y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        print(f\"DEBUG fit MLP with loss={loss.detach().cpu().item():.3f}\")\n",
    "    def predict(self, x):\n",
    "        self.eval()\n",
    "        return self(x)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reg_ids_oh.shape)\n",
    "print(reg_geom_feats.shape)\n",
    "print(sup_geom_feats.shape)\n",
    "print(all_feats_sup_trans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = model(\n",
    "#     reg_lut = reg_ids_oh.to(config_device,dtype=torch.float32),\n",
    "#     reg_geom = reg_geom_feats.to(config_device,dtype=torch.float32),\n",
    "#     sup_geom = sup_geom_feats.to(config_device,dtype=torch.float32),\n",
    "#     pts_geom = all_feats_sup_trans.to(config_device,dtype=torch.float32),\n",
    "# )\n",
    "config_device='cuda'\n",
    "in_all = dict(\n",
    "    reg_lut = reg_ids_oh.to(config_device,dtype=torch.float32),\n",
    "    reg_geom = reg_geom_feats.to(config_device,dtype=torch.float32),\n",
    "    sup_geom = sup_geom_feats.to(config_device,dtype=torch.float32),\n",
    "    pts_geom = all_feats_sup_trans.to(config_device,dtype=torch.float32),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,v in in_all.items():\n",
    "    print(n, v.shape)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<text color=\"red\">asd</text>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init results container\n",
    "results = dict()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min-cut solution\n",
    "import maxflow\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# informed picking\n",
    "# initial mask\n",
    "\n",
    "### CONFIG\n",
    "config_init_k = 3 # picks per class\n",
    "config_k_per_it = 2\n",
    "config_epochs = 1000\n",
    "num_iterations = 20\n",
    "num_experiments = 30\n",
    "\n",
    "# inputs\n",
    "V,F = all_feats_sup_trans.shape\n",
    "\n",
    "\n",
    "node_names = [ \n",
    "    # 'reg_geom',\n",
    "    'sup_geom',\n",
    "    'pts_geom',\n",
    "]\n",
    "exp_name = 'sup_geom+pts_geom+lut'\n",
    "# exp_name = 'sup_geom+pts_geom'\n",
    "# exp_name = 'pts_geom'\n",
    "# exp_name = 'sup_geom'\n",
    "\n",
    "\n",
    "\n",
    "results[exp_name] = list()\n",
    "\n",
    "for experiment_i in range(num_experiments):\n",
    "    results[exp_name].append(dict(\n",
    "        iou=list(),\n",
    "        acc=list()\n",
    "    ))\n",
    "\n",
    "    # initial mask (simulated picks)\n",
    "    mask_fg = torch.tensor(random.sample(torch.nonzero(sup_labels==0).tolist(),k=config_init_k))\n",
    "    mask_bg = torch.tensor(random.sample(torch.nonzero(sup_labels==1).tolist(),k=config_init_k))\n",
    "\n",
    "\n",
    "    mask_train = torch.zeros(sup_labels.shape[0],dtype=torch.int64).bool()\n",
    "    mask_train[mask_fg] = True\n",
    "    mask_train[mask_bg] = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    node_feats_t = torch.concat([v for k,v in in_all.items() if k in node_names],dim=-1)\n",
    "    node_feats = node_feats_t.detach().cpu().numpy()\n",
    "            \n",
    "    model = CompositeModel(\n",
    "        mlp_reg_lut = mlp_reg_lut, \n",
    "        # mlp_reg_geom = mlp_reg_geom,    \n",
    "        mlp_sup_geom = mlp_sup_geom,    \n",
    "        mlp_pts_geom = mlp_pts_geom,    \n",
    "    ).to(config_device)\n",
    "    enforce_region_majority_voting = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # filter initial features and labels\n",
    "    train_feats = {k:v[mask_train] for k, v in in_all.items()}\n",
    "    train_y = sup_labels[mask_train]\n",
    "\n",
    "    # grab graph info\n",
    "    first_edg = graph_f[0] # [V]\n",
    "    adj_verts = graph_f[1] # [E]\n",
    "\n",
    "    # solve\n",
    "    # iteration 0\n",
    "\n",
    "\n",
    "    # define iteration\n",
    "    def fit_and_solve(model,train_feats, train_y, num_epochs, sup_labels, node_feats_t, first_edg, adj_verts):\n",
    "\n",
    "        model.fit(train_feats, train_y,epochs=num_epochs)\n",
    "        preds = model(in_all).detach().cpu().numpy()\n",
    "        print(f\"Unary Accuracy: {((sup_labels.numpy() == np.argmax(preds, axis=1)).sum()/preds.shape[0]).item()*100:.3f}%\")\n",
    "        cost_source     = - preds[:,1]\n",
    "        cost_sink       = - preds[:,0]\n",
    "\n",
    "        beta,lmbda = calibrate_mrf(node_feats, cost_source, cost_sink, first_edg, adj_verts)\n",
    "        mc_preds = solve_mincut(node_feats, cost_source, cost_sink, first_edg, adj_verts, lmbda*1, beta*1)\n",
    "        if enforce_region_majority_voting:\n",
    "            mc_preds = np.eye(2)[mc_preds]\n",
    "            mc_preds = mc_preds[sup_f]\n",
    "            mc_preds = scatter_mean(torch.as_tensor(mc_preds,dtype=torch.float32), torch.as_tensor(reg_ids,dtype=torch.int64), dim=0)\n",
    "            mc_preds = np.eye(2)[torch.argmax(mc_preds, dim=1)[reg_ids]]\n",
    "            mc_preds = scatter_mean(torch.as_tensor(mc_preds,dtype=torch.float32), torch.as_tensor(sup_f,dtype=torch.int64), dim=0)\n",
    "            mc_preds = torch.argmax(mc_preds, dim=1).cpu().numpy()\n",
    "            \n",
    "            \n",
    "        acc = ((sup_labels.numpy() == mc_preds.astype(bool)).mean()).item()\n",
    "        print(f\"Accuracy: {acc*100:.3f}%\")\n",
    "        y_true = sup_labels.numpy().astype(bool)\n",
    "        y_pred = mc_preds.astype(bool)\n",
    "\n",
    "        intersection = np.logical_and(y_true, y_pred).sum()\n",
    "        union        = np.logical_or(y_true,  y_pred).sum()\n",
    "        iou = intersection / union if union > 0 else 0.0\n",
    "\n",
    "        print(f\"IoU: {iou*100:.3f}%\")\n",
    "        results[exp_name][-1]['iou'].append(iou)\n",
    "        results[exp_name][-1]['acc'].append(acc)\n",
    "        \n",
    "\n",
    "        return mc_preds, mc_preds == sup_labels.numpy()\n",
    "\n",
    "    # init solve\n",
    "    labels, correct = fit_and_solve(model,train_feats, train_y,config_epochs,sup_labels,node_feats_t,first_edg,adj_verts)\n",
    "    acc_best = 0\n",
    "    labels_best = None\n",
    "    for it in range(num_iterations):\n",
    "\n",
    "\n",
    "\n",
    "        # update train mask from wrong classifications\n",
    "        wrong_sup_idx = np.nonzero(~correct)[0]\n",
    "        try:\n",
    "            new_samples_fg = random.sample(wrong_sup_idx[sup_labels[wrong_sup_idx]==0].tolist(), k=config_k_per_it)\n",
    "            new_samples_bg = random.sample(wrong_sup_idx[sup_labels[wrong_sup_idx]==1].tolist(), k=config_k_per_it)\n",
    "        except ValueError as e:\n",
    "            print(f\"no more samples: {e}\")\n",
    "            continue\n",
    "        mask_train[new_samples_bg] = True\n",
    "        mask_train[new_samples_fg] = True\n",
    "        \n",
    "        # mask features\n",
    "        train_feats = {k:v[mask_train] for k, v in in_all.items()}\n",
    "        train_y = sup_labels[mask_train]\n",
    "        \n",
    "        # run iteration\n",
    "        labels, correct = fit_and_solve(model,train_feats, train_y, config_epochs, sup_labels, node_feats_t, first_edg, adj_verts)\n",
    "        # report metrics\n",
    "        acc_new = correct.mean()\n",
    "        if acc_new > acc_best:\n",
    "            acc_best = acc_new\n",
    "            labels_best = labels\n",
    "            print(f'>>>>>>> new best acc: {acc_best} <<<<<<<<<<<')\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    # plot results\n",
    "#     plotter10 = pv.Plotter(notebook=True)\n",
    "#     pcd_poly_mincut = pv.PolyData(pos)\n",
    "\n",
    "#     pcd_poly_mincut['s'] = labels_best[sup_f]\n",
    "\n",
    "\n",
    "\n",
    "# # plotter10.add_mesh(pcd_poly_mincut,scalars='s',rgb=False,cmap=['#ae8484','#c7e881'],categories=True)\n",
    "# # # update centroids to reflect mask\n",
    "# # sup_centroids_fg = sup_centroids[torch.logical_and(mask_train, sup_labels==0)]\n",
    "# # sup_centroids_bg = sup_centroids[torch.logical_and(mask_train, sup_labels==1)]\n",
    "\n",
    "# # for centroid in sup_centroids_fg:\n",
    "# #     sp = pv.Sphere(0.2,centroid.numpy())\n",
    "# #     plotter10.add_mesh(sp, color='f01c1c')\n",
    "\n",
    "# # for centroid in sup_centroids_bg:\n",
    "# #     sp = pv.Sphere(0.2,centroid.numpy())\n",
    "# #     plotter10.add_mesh(sp, color='#a8f209')\n",
    "\n",
    "# # plotter10.show()\n",
    "# # reset_camera_standard(plotter10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min-cut solution\n",
    "import maxflow\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# informed picking\n",
    "# initial mask\n",
    "\n",
    "### CONFIG\n",
    "config_init_k = 3 # picks per class\n",
    "config_k_per_it = 2\n",
    "config_epochs = 1000\n",
    "num_iterations = 20\n",
    "num_experiments = 30\n",
    "\n",
    "# inputs\n",
    "V,F = all_feats_sup_trans.shape\n",
    "\n",
    "\n",
    "node_names = [ \n",
    "    # 'reg_geom',\n",
    "    'sup_geom',\n",
    "    'pts_geom',\n",
    "]\n",
    "# exp_name = 'sup_geom+pts_geom+lut'\n",
    "exp_name = 'sup_geom+pts_geom'\n",
    "# exp_name = 'pts_geom'\n",
    "# exp_name = 'sup_geom'\n",
    "\n",
    "\n",
    "\n",
    "results[exp_name] = list()\n",
    "\n",
    "for experiment_i in range(num_experiments):\n",
    "    results[exp_name].append(dict(\n",
    "        iou=list(),\n",
    "        acc=list()\n",
    "    ))\n",
    "\n",
    "    # initial mask (simulated picks)\n",
    "    mask_fg = torch.tensor(random.sample(torch.nonzero(sup_labels==0).tolist(),k=config_init_k))\n",
    "    mask_bg = torch.tensor(random.sample(torch.nonzero(sup_labels==1).tolist(),k=config_init_k))\n",
    "\n",
    "\n",
    "    mask_train = torch.zeros(sup_labels.shape[0],dtype=torch.int64).bool()\n",
    "    mask_train[mask_fg] = True\n",
    "    mask_train[mask_bg] = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    node_feats_t = torch.concat([v for k,v in in_all.items() if k in node_names],dim=-1)\n",
    "    node_feats = node_feats_t.detach().cpu().numpy()\n",
    "            \n",
    "    model = CompositeModel(\n",
    "        # mlp_reg_lut = mlp_reg_lut, \n",
    "        # mlp_reg_geom = mlp_reg_geom,    \n",
    "        mlp_sup_geom = mlp_sup_geom,    \n",
    "        mlp_pts_geom = mlp_pts_geom,    \n",
    "    ).to(config_device)\n",
    "    enforce_region_majority_voting = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # filter initial features and labels\n",
    "    train_feats = {k:v[mask_train] for k, v in in_all.items()}\n",
    "    train_y = sup_labels[mask_train]\n",
    "\n",
    "    # grab graph info\n",
    "    first_edg = graph_f[0] # [V]\n",
    "    adj_verts = graph_f[1] # [E]\n",
    "\n",
    "    # solve\n",
    "    # iteration 0\n",
    "\n",
    "\n",
    "    # define iteration\n",
    "    def fit_and_solve(model,train_feats, train_y, num_epochs, sup_labels, node_feats_t, first_edg, adj_verts):\n",
    "\n",
    "        model.fit(train_feats, train_y,epochs=num_epochs)\n",
    "        preds = model(in_all).detach().cpu().numpy()\n",
    "        print(f\"Unary Accuracy: {((sup_labels.numpy() == np.argmax(preds, axis=1)).sum()/preds.shape[0]).item()*100:.3f}%\")\n",
    "        cost_source     = - preds[:,1]\n",
    "        cost_sink       = - preds[:,0]\n",
    "\n",
    "        beta,lmbda = calibrate_mrf(node_feats, cost_source, cost_sink, first_edg, adj_verts)\n",
    "        mc_preds = solve_mincut(node_feats, cost_source, cost_sink, first_edg, adj_verts, lmbda*1, beta*1)\n",
    "        if enforce_region_majority_voting:\n",
    "            mc_preds = np.eye(2)[mc_preds]\n",
    "            mc_preds = mc_preds[sup_f]\n",
    "            mc_preds = scatter_mean(torch.as_tensor(mc_preds,dtype=torch.float32), torch.as_tensor(reg_ids,dtype=torch.int64), dim=0)\n",
    "            mc_preds = np.eye(2)[torch.argmax(mc_preds, dim=1)[reg_ids]]\n",
    "            mc_preds = scatter_mean(torch.as_tensor(mc_preds,dtype=torch.float32), torch.as_tensor(sup_f,dtype=torch.int64), dim=0)\n",
    "            mc_preds = torch.argmax(mc_preds, dim=1).cpu().numpy()\n",
    "            \n",
    "            \n",
    "        acc = ((sup_labels.numpy() == mc_preds.astype(bool)).mean()).item()\n",
    "        print(f\"Accuracy: {acc*100:.3f}%\")\n",
    "        y_true = sup_labels.numpy().astype(bool)\n",
    "        y_pred = mc_preds.astype(bool)\n",
    "\n",
    "        intersection = np.logical_and(y_true, y_pred).sum()\n",
    "        union        = np.logical_or(y_true,  y_pred).sum()\n",
    "        iou = intersection / union if union > 0 else 0.0\n",
    "\n",
    "        print(f\"IoU: {iou*100:.3f}%\")\n",
    "        results[exp_name][-1]['iou'].append(iou)\n",
    "        results[exp_name][-1]['acc'].append(acc)\n",
    "        \n",
    "\n",
    "        return mc_preds, mc_preds == sup_labels.numpy()\n",
    "\n",
    "    # init solve\n",
    "    labels, correct = fit_and_solve(model,train_feats, train_y,config_epochs,sup_labels,node_feats_t,first_edg,adj_verts)\n",
    "    acc_best = 0\n",
    "    labels_best = None\n",
    "    for it in range(num_iterations):\n",
    "\n",
    "\n",
    "\n",
    "        # update train mask from wrong classifications\n",
    "        wrong_sup_idx = np.nonzero(~correct)[0]\n",
    "        try:\n",
    "            new_samples_fg = random.sample(wrong_sup_idx[sup_labels[wrong_sup_idx]==0].tolist(), k=config_k_per_it)\n",
    "            new_samples_bg = random.sample(wrong_sup_idx[sup_labels[wrong_sup_idx]==1].tolist(), k=config_k_per_it)\n",
    "        except ValueError as e:\n",
    "            print(f\"no more samples: {e}\")\n",
    "            continue\n",
    "        mask_train[new_samples_bg] = True\n",
    "        mask_train[new_samples_fg] = True\n",
    "        \n",
    "        # mask features\n",
    "        train_feats = {k:v[mask_train] for k, v in in_all.items()}\n",
    "        train_y = sup_labels[mask_train]\n",
    "        \n",
    "        # run iteration\n",
    "        labels, correct = fit_and_solve(model,train_feats, train_y, config_epochs, sup_labels, node_feats_t, first_edg, adj_verts)\n",
    "        # report metrics\n",
    "        acc_new = correct.mean()\n",
    "        if acc_new > acc_best:\n",
    "            acc_best = acc_new\n",
    "            labels_best = labels\n",
    "            print(f'>>>>>>> new best acc: {acc_best} <<<<<<<<<<<')\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min-cut solution\n",
    "import maxflow\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# informed picking\n",
    "# initial mask\n",
    "\n",
    "### CONFIG\n",
    "config_init_k = 3 # picks per class\n",
    "config_k_per_it = 2\n",
    "config_epochs = 1000\n",
    "num_iterations = 20\n",
    "num_experiments = 30\n",
    "\n",
    "# inputs\n",
    "V,F = all_feats_sup_trans.shape\n",
    "\n",
    "\n",
    "node_names = [ \n",
    "    # 'reg_geom',\n",
    "    # 'sup_geom',\n",
    "    'pts_geom',\n",
    "]\n",
    "# exp_name = 'sup_geom+pts_geom+lut'\n",
    "# exp_name = 'sup_geom+pts_geom'\n",
    "exp_name = 'pts_geom'\n",
    "# exp_name = 'sup_geom'\n",
    "\n",
    "\n",
    "\n",
    "results[exp_name] = list()\n",
    "\n",
    "for experiment_i in range(num_experiments):\n",
    "    results[exp_name].append(dict(\n",
    "        iou=list(),\n",
    "        acc=list()\n",
    "    ))\n",
    "\n",
    "    # initial mask (simulated picks)\n",
    "    mask_fg = torch.tensor(random.sample(torch.nonzero(sup_labels==0).tolist(),k=config_init_k))\n",
    "    mask_bg = torch.tensor(random.sample(torch.nonzero(sup_labels==1).tolist(),k=config_init_k))\n",
    "\n",
    "\n",
    "    mask_train = torch.zeros(sup_labels.shape[0],dtype=torch.int64).bool()\n",
    "    mask_train[mask_fg] = True\n",
    "    mask_train[mask_bg] = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    node_feats_t = torch.concat([v for k,v in in_all.items() if k in node_names],dim=-1)\n",
    "    node_feats = node_feats_t.detach().cpu().numpy()\n",
    "            \n",
    "    model = CompositeModel(\n",
    "        # mlp_reg_lut = mlp_reg_lut, \n",
    "        # mlp_reg_geom = mlp_reg_geom,    \n",
    "        # mlp_sup_geom = mlp_sup_geom,    \n",
    "        mlp_pts_geom = mlp_pts_geom,    \n",
    "    ).to(config_device)\n",
    "    enforce_region_majority_voting = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # filter initial features and labels\n",
    "    train_feats = {k:v[mask_train] for k, v in in_all.items()}\n",
    "    train_y = sup_labels[mask_train]\n",
    "\n",
    "    # grab graph info\n",
    "    first_edg = graph_f[0] # [V]\n",
    "    adj_verts = graph_f[1] # [E]\n",
    "\n",
    "    # solve\n",
    "    # iteration 0\n",
    "\n",
    "\n",
    "    # define iteration\n",
    "    def fit_and_solve(model,train_feats, train_y, num_epochs, sup_labels, node_feats_t, first_edg, adj_verts):\n",
    "\n",
    "        model.fit(train_feats, train_y,epochs=num_epochs)\n",
    "        preds = model(in_all).detach().cpu().numpy()\n",
    "        print(f\"Unary Accuracy: {((sup_labels.numpy() == np.argmax(preds, axis=1)).sum()/preds.shape[0]).item()*100:.3f}%\")\n",
    "        cost_source     = - preds[:,1]\n",
    "        cost_sink       = - preds[:,0]\n",
    "\n",
    "        beta,lmbda = calibrate_mrf(node_feats, cost_source, cost_sink, first_edg, adj_verts)\n",
    "        mc_preds = solve_mincut(node_feats, cost_source, cost_sink, first_edg, adj_verts, lmbda*1, beta*1)\n",
    "        if enforce_region_majority_voting:\n",
    "            mc_preds = np.eye(2)[mc_preds]\n",
    "            mc_preds = mc_preds[sup_f]\n",
    "            mc_preds = scatter_mean(torch.as_tensor(mc_preds,dtype=torch.float32), torch.as_tensor(reg_ids,dtype=torch.int64), dim=0)\n",
    "            mc_preds = np.eye(2)[torch.argmax(mc_preds, dim=1)[reg_ids]]\n",
    "            mc_preds = scatter_mean(torch.as_tensor(mc_preds,dtype=torch.float32), torch.as_tensor(sup_f,dtype=torch.int64), dim=0)\n",
    "            mc_preds = torch.argmax(mc_preds, dim=1).cpu().numpy()\n",
    "            \n",
    "            \n",
    "        acc = ((sup_labels.numpy() == mc_preds.astype(bool)).mean()).item()\n",
    "        print(f\"Accuracy: {acc*100:.3f}%\")\n",
    "        y_true = sup_labels.numpy().astype(bool)\n",
    "        y_pred = mc_preds.astype(bool)\n",
    "\n",
    "        intersection = np.logical_and(y_true, y_pred).sum()\n",
    "        union        = np.logical_or(y_true,  y_pred).sum()\n",
    "        iou = intersection / union if union > 0 else 0.0\n",
    "\n",
    "        print(f\"IoU: {iou*100:.3f}%\")\n",
    "        results[exp_name][-1]['iou'].append(iou)\n",
    "        results[exp_name][-1]['acc'].append(acc)\n",
    "        \n",
    "\n",
    "        return mc_preds, mc_preds == sup_labels.numpy()\n",
    "\n",
    "    # init solve\n",
    "    labels, correct = fit_and_solve(model,train_feats, train_y,config_epochs,sup_labels,node_feats_t,first_edg,adj_verts)\n",
    "    acc_best = 0\n",
    "    labels_best = None\n",
    "    for it in range(num_iterations):\n",
    "\n",
    "\n",
    "\n",
    "        # update train mask from wrong classifications\n",
    "        wrong_sup_idx = np.nonzero(~correct)[0]\n",
    "        try:\n",
    "            new_samples_fg = random.sample(wrong_sup_idx[sup_labels[wrong_sup_idx]==0].tolist(), k=config_k_per_it)\n",
    "            new_samples_bg = random.sample(wrong_sup_idx[sup_labels[wrong_sup_idx]==1].tolist(), k=config_k_per_it)\n",
    "        except ValueError as e:\n",
    "            print(f\"no more samples: {e}\")\n",
    "            continue\n",
    "        mask_train[new_samples_bg] = True\n",
    "        mask_train[new_samples_fg] = True\n",
    "        \n",
    "        # mask features\n",
    "        train_feats = {k:v[mask_train] for k, v in in_all.items()}\n",
    "        train_y = sup_labels[mask_train]\n",
    "        \n",
    "        # run iteration\n",
    "        labels, correct = fit_and_solve(model,train_feats, train_y, config_epochs, sup_labels, node_feats_t, first_edg, adj_verts)\n",
    "        # report metrics\n",
    "        acc_new = correct.mean()\n",
    "        if acc_new > acc_best:\n",
    "            acc_best = acc_new\n",
    "            labels_best = labels\n",
    "            print(f'>>>>>>> new best acc: {acc_best} <<<<<<<<<<<')\n",
    "            \n",
    "            \n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min-cut solution\n",
    "import maxflow\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# informed picking\n",
    "# initial mask\n",
    "\n",
    "### CONFIG\n",
    "config_init_k = 3 # picks per class\n",
    "config_k_per_it = 2\n",
    "config_epochs = 1000\n",
    "num_iterations = 20\n",
    "num_experiments = 30\n",
    "\n",
    "# inputs\n",
    "V,F = all_feats_sup_trans.shape\n",
    "\n",
    "\n",
    "node_names = [ \n",
    "    # 'reg_geom',\n",
    "    'sup_geom',\n",
    "    # 'pts_geom',\n",
    "]\n",
    "# exp_name = 'sup_geom+pts_geom+lut'\n",
    "# exp_name = 'sup_geom+pts_geom'\n",
    "# exp_name = 'pts_geom'\n",
    "exp_name = 'sup_geom'\n",
    "\n",
    "\n",
    "\n",
    "results[exp_name] = list()\n",
    "\n",
    "for experiment_i in range(num_experiments):\n",
    "    results[exp_name].append(dict(\n",
    "        iou=list(),\n",
    "        acc=list()\n",
    "    ))\n",
    "\n",
    "    # initial mask (simulated picks)\n",
    "    mask_fg = torch.tensor(random.sample(torch.nonzero(sup_labels==0).tolist(),k=config_init_k))\n",
    "    mask_bg = torch.tensor(random.sample(torch.nonzero(sup_labels==1).tolist(),k=config_init_k))\n",
    "\n",
    "\n",
    "    mask_train = torch.zeros(sup_labels.shape[0],dtype=torch.int64).bool()\n",
    "    mask_train[mask_fg] = True\n",
    "    mask_train[mask_bg] = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    node_feats_t = torch.concat([v for k,v in in_all.items() if k in node_names],dim=-1)\n",
    "    node_feats = node_feats_t.detach().cpu().numpy()\n",
    "            \n",
    "    model = CompositeModel(\n",
    "        # mlp_reg_lut = mlp_reg_lut, \n",
    "        # mlp_reg_geom = mlp_reg_geom,    \n",
    "        mlp_sup_geom = mlp_sup_geom,    \n",
    "        # mlp_pts_geom = mlp_pts_geom,    \n",
    "    ).to(config_device)\n",
    "    enforce_region_majority_voting = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # filter initial features and labels\n",
    "    train_feats = {k:v[mask_train] for k, v in in_all.items()}\n",
    "    train_y = sup_labels[mask_train]\n",
    "\n",
    "    # grab graph info\n",
    "    first_edg = graph_f[0] # [V]\n",
    "    adj_verts = graph_f[1] # [E]\n",
    "\n",
    "    # solve\n",
    "    # iteration 0\n",
    "\n",
    "\n",
    "    # define iteration\n",
    "    def fit_and_solve(model,train_feats, train_y, num_epochs, sup_labels, node_feats_t, first_edg, adj_verts):\n",
    "\n",
    "        model.fit(train_feats, train_y,epochs=num_epochs)\n",
    "        preds = model(in_all).detach().cpu().numpy()\n",
    "        print(f\"Unary Accuracy: {((sup_labels.numpy() == np.argmax(preds, axis=1)).sum()/preds.shape[0]).item()*100:.3f}%\")\n",
    "        cost_source     = - preds[:,1]\n",
    "        cost_sink       = - preds[:,0]\n",
    "\n",
    "        beta,lmbda = calibrate_mrf(node_feats, cost_source, cost_sink, first_edg, adj_verts)\n",
    "        mc_preds = solve_mincut(node_feats, cost_source, cost_sink, first_edg, adj_verts, lmbda*1, beta*1)\n",
    "        if enforce_region_majority_voting:\n",
    "            mc_preds = np.eye(2)[mc_preds]\n",
    "            mc_preds = mc_preds[sup_f]\n",
    "            mc_preds = scatter_mean(torch.as_tensor(mc_preds,dtype=torch.float32), torch.as_tensor(reg_ids,dtype=torch.int64), dim=0)\n",
    "            mc_preds = np.eye(2)[torch.argmax(mc_preds, dim=1)[reg_ids]]\n",
    "            mc_preds = scatter_mean(torch.as_tensor(mc_preds,dtype=torch.float32), torch.as_tensor(sup_f,dtype=torch.int64), dim=0)\n",
    "            mc_preds = torch.argmax(mc_preds, dim=1).cpu().numpy()\n",
    "            \n",
    "            \n",
    "        acc = ((sup_labels.numpy() == mc_preds.astype(bool)).mean()).item()\n",
    "        print(f\"Accuracy: {acc*100:.3f}%\")\n",
    "        y_true = sup_labels.numpy().astype(bool)\n",
    "        y_pred = mc_preds.astype(bool)\n",
    "\n",
    "        intersection = np.logical_and(y_true, y_pred).sum()\n",
    "        union        = np.logical_or(y_true,  y_pred).sum()\n",
    "        iou = intersection / union if union > 0 else 0.0\n",
    "\n",
    "        print(f\"IoU: {iou*100:.3f}%\")\n",
    "        results[exp_name][-1]['iou'].append(iou)\n",
    "        results[exp_name][-1]['acc'].append(acc)\n",
    "        \n",
    "\n",
    "        return mc_preds, mc_preds == sup_labels.numpy()\n",
    "\n",
    "    # init solve\n",
    "    labels, correct = fit_and_solve(model,train_feats, train_y,config_epochs,sup_labels,node_feats_t,first_edg,adj_verts)\n",
    "    acc_best = 0\n",
    "    labels_best = None\n",
    "    for it in range(num_iterations):\n",
    "\n",
    "\n",
    "\n",
    "        # update train mask from wrong classifications\n",
    "        wrong_sup_idx = np.nonzero(~correct)[0]\n",
    "        try:\n",
    "            new_samples_fg = random.sample(wrong_sup_idx[sup_labels[wrong_sup_idx]==0].tolist(), k=config_k_per_it)\n",
    "            new_samples_bg = random.sample(wrong_sup_idx[sup_labels[wrong_sup_idx]==1].tolist(), k=config_k_per_it)\n",
    "        except ValueError as e:\n",
    "            print(f\"no more samples: {e}\")\n",
    "            continue\n",
    "        mask_train[new_samples_bg] = True\n",
    "        mask_train[new_samples_fg] = True\n",
    "        \n",
    "        # mask features\n",
    "        train_feats = {k:v[mask_train] for k, v in in_all.items()}\n",
    "        train_y = sup_labels[mask_train]\n",
    "        \n",
    "        # run iteration\n",
    "        labels, correct = fit_and_solve(model,train_feats, train_y, config_epochs, sup_labels, node_feats_t, first_edg, adj_verts)\n",
    "        # report metrics\n",
    "        acc_new = correct.mean()\n",
    "        if acc_new > acc_best:\n",
    "            acc_best = acc_new\n",
    "            labels_best = labels\n",
    "            print(f'>>>>>>> new best acc: {acc_best} <<<<<<<<<<<')\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    # plot results\n",
    "    plotter10 = pv.Plotter(notebook=True)\n",
    "    pcd_poly_mincut = pv.PolyData(pos)\n",
    "\n",
    "    pcd_poly_mincut['s'] = labels_best[sup_f]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG fit MLP with loss=0.002\n",
      "Unary Accuracy: 69.482%\n",
      "DEBUG solved mincut with flow=9111.510503858706\n",
      "Accuracy: 69.021%\n",
      "IoU: 30.730%\n",
      "DEBUG fit MLP with loss=0.000\n",
      "Unary Accuracy: 75.198%\n",
      "DEBUG solved mincut with flow=25451.901141624818\n",
      "Accuracy: 77.560%\n",
      "IoU: 36.413%\n",
      ">>>>>>> new best acc: 0.7756022695563204 <<<<<<<<<<<\n",
      "DEBUG fit MLP with loss=0.035\n",
      "Unary Accuracy: 72.519%\n",
      "DEBUG solved mincut with flow=10547.6419455487\n",
      "Accuracy: 75.281%\n",
      "IoU: 42.794%\n"
     ]
    }
   ],
   "source": [
    "# min-cut solution\n",
    "import maxflow\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# informed picking\n",
    "# initial mask\n",
    "\n",
    "### CONFIG\n",
    "config_init_k = 3 # picks per class\n",
    "config_k_per_it = 2\n",
    "config_epochs = 1000\n",
    "num_iterations = 20\n",
    "num_experiments = 30\n",
    "\n",
    "# inputs\n",
    "V,F = all_feats_sup_trans.shape\n",
    "\n",
    "\n",
    "node_names = [ \n",
    "    'reg_geom',\n",
    "    'sup_geom',\n",
    "    'pts_geom',\n",
    "]\n",
    "exp_name = 'sup_geom+pts_geom+lut+regnode'\n",
    "# exp_name = 'sup_geom+pts_geom+lut'\n",
    "# exp_name = 'sup_geom+pts_geom'\n",
    "# exp_name = 'pts_geom'\n",
    "# exp_name = 'sup_geom'\n",
    "\n",
    "\n",
    "\n",
    "results[exp_name] = list()\n",
    "\n",
    "for experiment_i in range(num_experiments):\n",
    "    results[exp_name].append(dict(\n",
    "        iou=list(),\n",
    "        acc=list()\n",
    "    ))\n",
    "\n",
    "    # initial mask (simulated picks)\n",
    "    mask_fg = torch.tensor(random.sample(torch.nonzero(sup_labels==0).tolist(),k=config_init_k))\n",
    "    mask_bg = torch.tensor(random.sample(torch.nonzero(sup_labels==1).tolist(),k=config_init_k))\n",
    "\n",
    "\n",
    "    mask_train = torch.zeros(sup_labels.shape[0],dtype=torch.int64).bool()\n",
    "    mask_train[mask_fg] = True\n",
    "    mask_train[mask_bg] = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    node_feats_t = torch.concat([v for k,v in in_all.items() if k in node_names],dim=-1)\n",
    "    node_feats = node_feats_t.detach().cpu().numpy()\n",
    "            \n",
    "    model = CompositeModel(\n",
    "        mlp_reg_lut = mlp_reg_lut, \n",
    "        mlp_reg_geom = mlp_reg_geom,    \n",
    "        mlp_sup_geom = mlp_sup_geom,    \n",
    "        mlp_pts_geom = mlp_pts_geom,    \n",
    "    ).to(config_device)\n",
    "    enforce_region_majority_voting = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # filter initial features and labels\n",
    "    train_feats = {k:v[mask_train] for k, v in in_all.items()}\n",
    "    train_y = sup_labels[mask_train]\n",
    "\n",
    "    # grab graph info\n",
    "    first_edg = graph_f[0] # [V]\n",
    "    adj_verts = graph_f[1] # [E]\n",
    "\n",
    "    # solve\n",
    "    # iteration 0\n",
    "\n",
    "\n",
    "    # define iteration\n",
    "    def fit_and_solve(model,train_feats, train_y, num_epochs, sup_labels, node_feats_t, first_edg, adj_verts):\n",
    "\n",
    "        model.fit(train_feats, train_y,epochs=num_epochs)\n",
    "        preds = model(in_all).detach().cpu().numpy()\n",
    "        print(f\"Unary Accuracy: {((sup_labels.numpy() == np.argmax(preds, axis=1)).sum()/preds.shape[0]).item()*100:.3f}%\")\n",
    "        cost_source     = - preds[:,1]\n",
    "        cost_sink       = - preds[:,0]\n",
    "\n",
    "        beta,lmbda = calibrate_mrf(node_feats, cost_source, cost_sink, first_edg, adj_verts)\n",
    "        mc_preds = solve_mincut(node_feats, cost_source, cost_sink, first_edg, adj_verts, lmbda*1, beta*1)\n",
    "        if enforce_region_majority_voting:\n",
    "            mc_preds = np.eye(2)[mc_preds]\n",
    "            mc_preds = mc_preds[sup_f]\n",
    "            mc_preds = scatter_mean(torch.as_tensor(mc_preds,dtype=torch.float32), torch.as_tensor(reg_ids,dtype=torch.int64), dim=0)\n",
    "            mc_preds = np.eye(2)[torch.argmax(mc_preds, dim=1)[reg_ids]]\n",
    "            mc_preds = scatter_mean(torch.as_tensor(mc_preds,dtype=torch.float32), torch.as_tensor(sup_f,dtype=torch.int64), dim=0)\n",
    "            mc_preds = torch.argmax(mc_preds, dim=1).cpu().numpy()\n",
    "            \n",
    "            \n",
    "        acc = ((sup_labels.numpy() == mc_preds.astype(bool)).mean()).item()\n",
    "        print(f\"Accuracy: {acc*100:.3f}%\")\n",
    "        y_true = sup_labels.numpy().astype(bool)\n",
    "        y_pred = mc_preds.astype(bool)\n",
    "\n",
    "        intersection = np.logical_and(y_true, y_pred).sum()\n",
    "        union        = np.logical_or(y_true,  y_pred).sum()\n",
    "        iou = intersection / union if union > 0 else 0.0\n",
    "\n",
    "        print(f\"IoU: {iou*100:.3f}%\")\n",
    "        results[exp_name][-1]['iou'].append(iou)\n",
    "        results[exp_name][-1]['acc'].append(acc)\n",
    "        \n",
    "\n",
    "        return mc_preds, mc_preds == sup_labels.numpy()\n",
    "\n",
    "    # init solve\n",
    "    labels, correct = fit_and_solve(model,train_feats, train_y,config_epochs,sup_labels,node_feats_t,first_edg,adj_verts)\n",
    "    acc_best = 0\n",
    "    labels_best = None\n",
    "    for it in range(num_iterations):\n",
    "\n",
    "\n",
    "\n",
    "        # update train mask from wrong classifications\n",
    "        wrong_sup_idx = np.nonzero(~correct)[0]\n",
    "        try:\n",
    "            new_samples_fg = random.sample(wrong_sup_idx[sup_labels[wrong_sup_idx]==0].tolist(), k=config_k_per_it)\n",
    "            new_samples_bg = random.sample(wrong_sup_idx[sup_labels[wrong_sup_idx]==1].tolist(), k=config_k_per_it)\n",
    "        except ValueError as e:\n",
    "            print(f\"no more samples: {e}\")\n",
    "            continue\n",
    "        mask_train[new_samples_bg] = True\n",
    "        mask_train[new_samples_fg] = True\n",
    "        \n",
    "        # mask features\n",
    "        train_feats = {k:v[mask_train] for k, v in in_all.items()}\n",
    "        train_y = sup_labels[mask_train]\n",
    "        \n",
    "        # run iteration\n",
    "        labels, correct = fit_and_solve(model,train_feats, train_y, config_epochs, sup_labels, node_feats_t, first_edg, adj_verts)\n",
    "        # report metrics\n",
    "        acc_new = correct.mean()\n",
    "        if acc_new > acc_best:\n",
    "            acc_best = acc_new\n",
    "            labels_best = labels\n",
    "            print(f'>>>>>>> new best acc: {acc_best} <<<<<<<<<<<')\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    # plot results\n",
    "#     plotter10 = pv.Plotter(notebook=True)\n",
    "#     pcd_poly_mincut = pv.PolyData(pos)\n",
    "\n",
    "#     pcd_poly_mincut['s'] = labels_best[sup_f]\n",
    "\n",
    "\n",
    "\n",
    "# # plotter10.add_mesh(pcd_poly_mincut,scalars='s',rgb=False,cmap=['#ae8484','#c7e881'],categories=True)\n",
    "# # # update centroids to reflect mask\n",
    "# # sup_centroids_fg = sup_centroids[torch.logical_and(mask_train, sup_labels==0)]\n",
    "# # sup_centroids_bg = sup_centroids[torch.logical_and(mask_train, sup_labels==1)]\n",
    "\n",
    "# # for centroid in sup_centroids_fg:\n",
    "# #     sp = pv.Sphere(0.2,centroid.numpy())\n",
    "# #     plotter10.add_mesh(sp, color='f01c1c')\n",
    "\n",
    "# # for centroid in sup_centroids_bg:\n",
    "# #     sp = pv.Sphere(0.2,centroid.numpy())\n",
    "# #     plotter10.add_mesh(sp, color='#a8f209')\n",
    "\n",
    "# # plotter10.show()\n",
    "# # reset_camera_standard(plotter10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./experiments/results_nb.pkl','wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "fig,ax = plt.subplots(1,1,figsize=(12,8))\n",
    "for ename, eres in results.items():\n",
    "    ious = []\n",
    "    accs = []\n",
    "    for i in  range(len(eres)):\n",
    "        if len(eres[i]['iou']) != num_iterations+1:\n",
    "            continue\n",
    "        ious.append(eres[i]['iou'])\n",
    "        accs.append(eres[i]['acc'])\n",
    "    accs = np.array(accs)\n",
    "    ax.plot(range(accs.shape[1]),accs.mean(0), label=ename)\n",
    "    ax.fill_between(range(accs.shape[1]),accs.mean(0)-accs.std(0),accs.mean(0)+accs.std(0),alpha=0.1)\n",
    "ax.set_title(\"Results tunnel\")\n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3dapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
