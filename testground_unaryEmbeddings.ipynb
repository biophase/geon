{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_scatter import scatter_mean\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import global_max_pool\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from glob import glob\n",
    "import random\n",
    "\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "from plyfile import PlyData, PlyElement\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "data_dir = f\"./data/FWF_flat/10.0X10.0\"\n",
    "osp.exists(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/FWF_flat/10.0X10.0/ply/Area_01_X000-Y007.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X001-Y005.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X001-Y006.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X001-Y007.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X002-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X002-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X002-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X002-Y005.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X002-Y006.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X002-Y007.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X002-Y008.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X003-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X003-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X003-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X003-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X003-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X003-Y005.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X003-Y006.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X003-Y007.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X004-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X004-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X004-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X004-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X004-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X004-Y005.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X004-Y006.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X004-Y007.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X005-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X005-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X005-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X005-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X005-Y005.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X005-Y006.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X005-Y007.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X006-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_01_X006-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_02_X000-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_02_X000-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_02_X000-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_02_X001-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_02_X001-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_02_X001-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_02_X001-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_02_X001-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_02_X002-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_02_X002-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_02_X002-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_02_X002-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_03_X000-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_03_X000-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_03_X001-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_04_X000-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_04_X000-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_05_X000-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_05_X000-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_05_X001-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_05_X001-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_05_X002-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_05_X002-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_07_X000-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_08_X000-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_08_X000-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_08_X000-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_08_X000-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_08_X000-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_08_X001-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_08_X001-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_08_X001-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_08_X001-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_08_X001-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_08_X002-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_08_X002-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_08_X002-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_08_X002-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_08_X003-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_09_X000-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_09_X000-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_09_X000-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_09_X000-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_09_X000-Y005.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_09_X000-Y006.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_09_X000-Y007.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_09_X000-Y008.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_09_X001-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_09_X001-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_09_X001-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_09_X001-Y005.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_09_X001-Y006.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_09_X001-Y007.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_09_X001-Y008.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_09_X001-Y009.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_09_X001-Y010.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_09_X001-Y011.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_09_X002-Y008.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_09_X002-Y009.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_09_X002-Y010.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_09_X002-Y011.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_09_X003-Y009.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_09_X003-Y010.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_09_X003-Y011.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_09_X004-Y010.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_09_X004-Y011.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_09_X005-Y010.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_09_X005-Y011.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_09_X005-Y012.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_10_X001-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_10_X001-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_10_X002-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_10_X002-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_10_X002-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_10_X003-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_10_X003-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_10_X003-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_10_X003-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_10_X004-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_10_X004-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_10_X004-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_10_X004-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_10_X004-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_10_X005-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_10_X005-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_10_X005-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_10_X005-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_10_X006-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_10_X006-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_10_X006-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_10_X007-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_10_X007-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_10_X008-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_11_X000-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_11_X000-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_11_X000-Y005.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_11_X000-Y006.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_11_X000-Y007.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_11_X000-Y008.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_11_X001-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_11_X001-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_11_X001-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_11_X001-Y005.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_11_X001-Y006.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_11_X001-Y007.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_11_X002-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_11_X002-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_11_X002-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_11_X002-Y005.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_11_X003-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_11_X003-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_11_X003-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_11_X003-Y005.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_11_X004-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_11_X004-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_11_X004-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_11_X005-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_11_X005-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_11_X005-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_11_X005-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_11_X006-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_11_X006-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_11_X006-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_12_X000-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_12_X000-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_12_X000-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_12_X001-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_12_X001-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_12_X001-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_12_X002-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_12_X002-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_12_X003-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_12_X003-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_12_X003-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_12_X003-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_12_X004-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_12_X004-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_12_X004-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_12_X005-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_12_X005-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_12_X005-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_12_X005-Y005.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_12_X006-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_13_X000-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_13_X000-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_13_X000-Y005.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_13_X001-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_13_X001-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_13_X001-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_13_X001-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_13_X001-Y005.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_13_X002-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_13_X002-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_13_X002-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_13_X002-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_13_X002-Y005.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_13_X003-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_13_X003-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_13_X003-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_13_X003-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_13_X004-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_13_X004-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_13_X004-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_13_X004-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_13_X004-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_13_X005-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_13_X005-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_13_X005-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_13_X005-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_13_X006-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_13_X006-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_13_X006-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_13_X007-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_13_X007-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X000-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X000-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X001-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X001-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X001-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X002-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X002-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X002-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X003-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X003-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X003-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X003-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X003-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X003-Y005.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X004-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X004-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X004-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X004-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X004-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X004-Y005.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X004-Y006.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X005-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X005-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X005-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X005-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X005-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X005-Y005.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X006-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X006-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X006-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X006-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X007-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X007-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X007-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_14_X008-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X000-Y005.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X000-Y006.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X001-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X001-Y005.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X001-Y006.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X001-Y007.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X002-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X002-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X002-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X002-Y005.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X002-Y006.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X002-Y007.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X002-Y008.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X003-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X003-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X003-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X003-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X003-Y005.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X003-Y006.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X003-Y007.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X003-Y008.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X004-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X004-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X004-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X004-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X004-Y004.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X004-Y005.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X004-Y006.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X004-Y007.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X005-Y000.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X005-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X005-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X005-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X005-Y005.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X006-Y001.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X006-Y002.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X006-Y003.ply',\n",
       " './data/FWF_flat/10.0X10.0/ply/Area_15_X007-Y002.ply']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate train_test split\n",
    "all_names = list(glob(osp.join(data_dir,'ply','*.ply')))\n",
    "\n",
    "# only train\n",
    "train_names = [n for n in all_names if 'Area_06' not in osp.basename(n)]\n",
    "train_names\n",
    "\n",
    "## split train/test\n",
    "# train_ratio = 0.95\n",
    "# N = len(all_names)\n",
    "# k = int(train_ratio * N)\n",
    "# train_i = random.sample(range(N), k) \n",
    "\n",
    "# train_mask = np.zeros(len(all_names)).astype(bool)\n",
    "# train_mask[train_i] = True\n",
    "\n",
    "\n",
    "# all_names = np.array(all_names)\n",
    "# train_names = all_names[train_mask].tolist()\n",
    "# test_names = all_names[~train_mask].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forwardstar_to_idx(first_edg:NDArray, adj_verts:NDArray)->NDArray:\n",
    "    sources = []\n",
    "    targets = []\n",
    "\n",
    "    V = len(first_edg) - 1\n",
    "    for u in range(V):\n",
    "        start, end = first_edg[u], first_edg[u+1]\n",
    "        src = np.full(end - start, u)\n",
    "        tgt = adj_verts[start:end]\n",
    "        sources.append(src)\n",
    "        targets.append(tgt)\n",
    "    sources = np.concat(sources)\n",
    "    targets = np.concat(targets)\n",
    "\n",
    "    return np.concat([sources[None,:], targets[None,:]],axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/288 [00:00<00:16, 17.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:23<00:00, 12.29it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 82\u001b[39m\n\u001b[32m     74\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.data)\n\u001b[32m     81\u001b[39m train_ds = ChunkedDataset(data_dir,[osp.basename(n).replace(\u001b[33m'\u001b[39m\u001b[33m.ply\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m train_names],num_classes=\u001b[32m89\u001b[39m) \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m test_ds = ChunkedDataset(data_dir,[osp.basename(n).replace(\u001b[33m'\u001b[39m\u001b[33m.ply\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtest_names\u001b[49m],num_classes=\u001b[32m89\u001b[39m) \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'test_names' is not defined"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "from torch_scatter import scatter_add\n",
    "\n",
    "def make_directed(edge_index: torch.Tensor) -> torch.Tensor:\n",
    "    rev = edge_index[[1, 0], :]\n",
    "    return torch.cat([edge_index, rev], dim=1)\n",
    "\n",
    "def labels_pt2reg(y:torch.Tensor, assignment:torch.Tensor, num_classes:int,dim_size:int|None=None)->torch.Tensor:\n",
    "    assert torch.all(y>=0)\n",
    "    y_oh = torch.eye(num_classes)\n",
    "    y_sum = scatter_add(y_oh[y], assignment, dim=0,dim_size=dim_size)\n",
    "    y_reg = torch.argmax(y_sum,dim=1)\n",
    "    return y_reg\n",
    "\n",
    "class ChunkedDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 data_dir:str,\n",
    "                 names:List[str],\n",
    "                 num_classes:int,\n",
    "                 ):\n",
    "        \n",
    "        self.data_dir = data_dir\n",
    "        self.data = list()\n",
    "        self.feat_names = list()\n",
    "        self.names = names\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        for pcd_fp in tqdm(glob(osp.join(data_dir,'ply','*.ply'))):\n",
    "            fn = osp.basename(pcd_fp).replace('.ply','')\n",
    "            if fn not in self.names:\n",
    "                continue\n",
    "            graph_fp = osp.join(osp.split(pcd_fp)[0],'..','graph',f\"{fn}.pkl\")\n",
    "\n",
    "            # load data\n",
    "            pcd = pd.DataFrame(PlyData.read(pcd_fp).elements[0].data)\n",
    "            pos = pcd[['x','y','z']].to_numpy(dtype=np.float32)\n",
    "            feat_names = [n for n in pcd.columns if n not in ['x','y','z','labels']]\n",
    "            if not len(self.feat_names): self.feat_names = feat_names\n",
    "            feats = pcd[feat_names].to_numpy(dtype=np.float32)\n",
    "            labels = pcd['labels'].to_numpy(dtype=np.int32)\n",
    "\n",
    "            \n",
    "            with open(graph_fp,'rb') as f:\n",
    "                graph = pickle.load(f)\n",
    "\n",
    "            \n",
    "            point_feats = graph['point_feats']\n",
    "\n",
    "            # edges\n",
    "            efs = graph['edges_forwardstar']                       \n",
    "            edge_index = forwardstar_to_idx(efs[0],efs[1])\n",
    "            edge_index = torch.as_tensor(edge_index,dtype=torch.int64)\n",
    "            edge_index = make_directed(edge_index)\n",
    "\n",
    "            # majority vote label for each superpoint\n",
    "            labels = torch.as_tensor(labels,dtype=torch.int64)\n",
    "            superpoint_idx = torch.as_tensor(graph['superpoint_idx'],dtype=torch.int64)\n",
    "            labels = labels_pt2reg(labels,assignment=superpoint_idx, num_classes=self.num_classes)\n",
    "\n",
    "            self.data.append(dict(\n",
    "                fn = fn,\n",
    "                pos = torch.as_tensor(pos),\n",
    "                feats = torch.as_tensor(feats),\n",
    "                labels = labels,\n",
    "                superpoint_idx = superpoint_idx,\n",
    "                edge_index = edge_index,\n",
    "                point_feats = torch.as_tensor(point_feats),\n",
    "            ))\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "train_ds = ChunkedDataset(data_dir,[osp.basename(n).replace('.ply','') for n in train_names],num_classes=89) # type: ignore\n",
    "# test_ds = ChunkedDataset(data_dir,[osp.basename(n).replace('.ply','') for n in test_names],num_classes=89) # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNetEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_dim:int,\n",
    "                 latent_dim:int,\n",
    "                 hidden: int = 64,\n",
    "                 dropout: float=0.1\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lin1   = nn.Linear(in_dim, hidden)\n",
    "        self.bn1    = nn.LayerNorm(hidden)\n",
    "        \n",
    "\n",
    "        self.lin2   = nn.Linear(hidden, latent_dim)\n",
    "        self.bn2    = nn.LayerNorm(latent_dim)\n",
    "\n",
    "        self.drop   = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, \n",
    "                pos: torch.Tensor,\n",
    "                point_feats: torch.Tensor,\n",
    "                super_idx: torch.Tensor                \n",
    "                ):\n",
    "\n",
    "        # prepare\n",
    "\n",
    "        x = torch.cat([pos, point_feats], dim=-1)\n",
    "        \n",
    "        # layer 1\n",
    "        x = self.drop(F.relu(self.bn1(self.lin1(x))))\n",
    "        # layer 2\n",
    "        x = self.drop(F.relu(self.bn2(self.lin2(x))))\n",
    "\n",
    "        # pool\n",
    "        super_embedding = global_max_pool(x, super_idx)\n",
    "\n",
    "        return super_embedding\n",
    "    \n",
    "class SPConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d:int,\n",
    "                 d_e:int):\n",
    "        super().__init__()\n",
    "        \n",
    "        # edge encoder\n",
    "        self.phi_e = nn.Sequential(\n",
    "            nn.Linear(3, d_e), nn.ReLU(),\n",
    "            nn.Linear(d_e, d_e), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # message MLP\n",
    "        self.phi_m = nn.Sequential(\n",
    "            nn.Linear(2*d + d_e, d), nn.ReLU(),\n",
    "            nn.Linear(d,d)\n",
    "        )\n",
    "\n",
    "        # update MLP\n",
    "        self.phi_u = nn.Sequential(\n",
    "            nn.Linear(2*d,d) , nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, \n",
    "                z,          # [R,d]\n",
    "                centroids,  # [R,3]\n",
    "                edge_index  # [2,E]\n",
    "                ):\n",
    "        src, dst = edge_index \n",
    "\n",
    "        # edge feats\n",
    "        delta = centroids[dst] - centroids[src] # [E,3]\n",
    "        e = self.phi_e(delta)                   # [E, d_3]\n",
    "\n",
    "        # messages\n",
    "        m = self.phi_m(torch.cat([z[src], z[dst], e], dim=-1))  # [E,d]\n",
    "\n",
    "        # aggregation\n",
    "        M = scatter_mean(m, dst, dim=0, dim_size=z.size(0))     # [R,d]\n",
    "\n",
    "        # update\n",
    "        out = self.phi_u(torch.cat([z,M], dim=-1))  # [R,d]\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_dim:int,\n",
    "                 num_classes:int,\n",
    "\n",
    "                 pn_latent_dim:int = 128,\n",
    "                 pn_hidden: int = 128,\n",
    "                 pn_dropout: float=0.1,\n",
    "\n",
    "                 super_hidden:int = 128,\n",
    "                 super_latent_dim:int = 32,\n",
    "                 super_dropout: float = 0.1,\n",
    "\n",
    "                 ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.encoder = PointNetEncoder(in_dim,pn_latent_dim,pn_hidden,pn_dropout)\n",
    "\n",
    "        self.lin1   = nn.Linear(pn_latent_dim,super_hidden)\n",
    "        self.bn1    = nn.LayerNorm(super_hidden)\n",
    "\n",
    "        self.lin2   = nn.Linear(super_hidden, super_latent_dim)\n",
    "        self.bn2    = nn.LayerNorm(super_latent_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(super_dropout)\n",
    "\n",
    "        self.classifier = nn.Linear(super_latent_dim, num_classes)\n",
    "\n",
    "    def forward(self,\n",
    "                pos: torch.Tensor,\n",
    "                point_feats: torch.Tensor,\n",
    "                super_idx: torch.Tensor                \n",
    "                ):\n",
    "        # prepare inputs\n",
    "        super_centroids = scatter_mean(pos, super_idx, dim=0)\n",
    "        pos_local = pos - super_centroids[super_idx]\n",
    "\n",
    "        # run model\n",
    "        x = self.encoder(pos_local, point_feats, super_idx)\n",
    "\n",
    "        x = self.dropout(F.relu(self.bn1(self.lin1(x))))\n",
    "        z = self.dropout(F.relu(self.bn2(self.lin2(x))))\n",
    "\n",
    "        return self.classifier(z), z\n",
    "\n",
    "\n",
    "class SPConvModel(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim:int,\n",
    "            num_classes:int,\n",
    "\n",
    "            pn_hidden: int = 128,\n",
    "            pn_dropout: float=0.1,\n",
    "\n",
    "            super_hidden:int=128,\n",
    "            super_dropout:float=0.2,\n",
    "                \n",
    "            edge_dim:int = 64,\n",
    "            super_num_conv:int=3,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.encoder = PointNetEncoder(in_dim,super_hidden,pn_hidden,pn_dropout)\n",
    "\n",
    "        self.conv_blocks = nn.ModuleList([\n",
    "            SPConv(super_hidden, edge_dim) for _ in range(super_num_conv)\n",
    "        ])\n",
    "        self.ln_blocks = nn.ModuleList([\n",
    "            nn.LayerNorm(super_hidden) for _ in range(super_num_conv)\n",
    "        ])\n",
    "        \n",
    "        self.classifier = nn.Linear(super_hidden,num_classes)\n",
    "        self.dropout = nn.Dropout(super_dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                pos: torch.Tensor,\n",
    "                point_feats: torch.Tensor,\n",
    "                super_idx: torch.Tensor,\n",
    "                edge_idx: torch.Tensor,                \n",
    "                ):\n",
    "        # prepare inputs\n",
    "        super_centroids = scatter_mean(pos, super_idx, dim=0)\n",
    "        pos_local = pos - super_centroids[super_idx]\n",
    "\n",
    "        # run model\n",
    "        x = self.encoder(pos_local, point_feats, super_idx)\n",
    "        z = x\n",
    "        for conv, ln in zip(self.conv_blocks, self.ln_blocks):\n",
    "            x = conv(x, super_centroids, edge_idx)\n",
    "            x = ln(x + z)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            z = x\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x, z\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 001: 100%|██████████| 273/273 [00:13<00:00, 20.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01  average loss: 2.8285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 002: 100%|██████████| 273/273 [00:12<00:00, 21.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02  average loss: 2.1180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 003: 100%|██████████| 273/273 [00:12<00:00, 21.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03  average loss: 1.8936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 004: 100%|██████████| 273/273 [00:12<00:00, 21.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04  average loss: 1.7923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 005: 100%|██████████| 273/273 [00:13<00:00, 20.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05  average loss: 1.7445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 006: 100%|██████████| 273/273 [00:13<00:00, 20.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06  average loss: 1.6806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 007: 100%|██████████| 273/273 [00:12<00:00, 21.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07  average loss: 1.6480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 008: 100%|██████████| 273/273 [00:13<00:00, 20.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08  average loss: 1.5978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 009:  70%|███████   | 192/273 [00:08<00:03, 23.47it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     41\u001b[39m         loss.backward()\n\u001b[32m     42\u001b[39m         optim.step()\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m         total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  average loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss/\u001b[38;5;28mlen\u001b[39m(train_dl)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m torch.save(model, \u001b[33m'\u001b[39m\u001b[33mspconv_contrastive_L0-1.pth\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# contrastive loss spconv\n",
    "\n",
    "config_max_epochs = 360\n",
    "config_device = 'cuda'\n",
    "config_margin = 1.0\n",
    "config_lambda_c = 0.1\n",
    "\n",
    "train_dl = DataLoader(train_ds,batch_size=1,shuffle=True, collate_fn=lambda x: x[0])\n",
    "# test_dl = DataLoader(test_ds,batch_size=1, collate_fn=lambda x: x[0])\n",
    "\n",
    "model = SimpleModel(\n",
    "    in_dim = 3+12,\n",
    "    num_classes=89,\n",
    "    pn_hidden=64,\n",
    "    super_hidden=64,\n",
    "    pn_dropout=0.1,\n",
    "    super_dropout=0.2,\n",
    "\n",
    ").to(device=config_device)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "for epoch in range(1,config_max_epochs+1):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    for d in tqdm(train_dl,total=len(train_ds), desc=f\"Epoch {epoch:03d}\"):\n",
    "        pos         = d['pos'].to(device=config_device)\n",
    "        point_feats = d['point_feats'].to(device=config_device)\n",
    "        labels      = d['labels'].to(device=config_device)\n",
    "        super_idx   = d['superpoint_idx'].to(device=config_device)\n",
    "\n",
    "\n",
    "        S = super_idx.amax() + 1 \n",
    "        logits, z = model(pos, point_feats, super_idx)\n",
    "        L_class = F.cross_entropy(logits, labels)\n",
    "\n",
    "\n",
    "        # total loss\n",
    "        loss = L_class \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}  average loss: {total_loss/len(train_dl):.4f}\")\n",
    "\n",
    "torch.save(model, './experiments/simple_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 001:   0%|          | 0/273 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 001: 100%|██████████| 273/273 [00:12<00:00, 21.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01  average loss: 4.4645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 002: 100%|██████████| 273/273 [00:13<00:00, 20.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02  average loss: 3.1270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 003: 100%|██████████| 273/273 [00:13<00:00, 20.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03  average loss: 2.8939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# contrastive loss spconv\n",
    "\n",
    "config_max_epochs = 360\n",
    "config_device = 'cuda'\n",
    "config_margin = 1.0\n",
    "config_lambda_c = 0.1\n",
    "\n",
    "train_dl = DataLoader(train_ds,batch_size=1,shuffle=True, collate_fn=lambda x: x[0])\n",
    "test_dl = DataLoader(test_ds,batch_size=1, collate_fn=lambda x: x[0])\n",
    "\n",
    "model = SPConvModel(\n",
    "    in_dim = 3+12,\n",
    "    num_classes=89,\n",
    "    pn_hidden=64,\n",
    "    super_hidden=64,\n",
    "    pn_dropout=0.1,\n",
    "    super_dropout=0.2,\n",
    "    edge_dim=32,\n",
    "    super_num_conv=3\n",
    ").to(device=config_device)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "for epoch in range(1,config_max_epochs+1):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    for d in tqdm(train_dl,total=len(train_ds), desc=f\"Epoch {epoch:03d}\"):\n",
    "        pos         = d['pos'].to(device=config_device)\n",
    "        point_feats = d['point_feats'].to(device=config_device)\n",
    "        labels      = d['labels'].to(device=config_device)\n",
    "        super_idx   = d['superpoint_idx'].to(device=config_device)\n",
    "        edge_idx    = d['edge_index'].to(device=config_device)\n",
    "\n",
    "        S = super_idx.amax() + 1 \n",
    "        logits, z = model(pos, point_feats, super_idx, edge_idx)\n",
    "        L_class = F.cross_entropy(logits, labels)\n",
    "\n",
    "        # contrastive loss\n",
    "        src, dst = edge_idx\n",
    "        zi, zj = z[src], z[dst]\n",
    "        dist2 = (zi - zj).pow(2).sum(dim=1)\n",
    "        same = (labels[src] == labels[dst]).float() # [E]\n",
    "        # pull\n",
    "        loss_pos = same * dist2\n",
    "        # push\n",
    "        loss_neg = (1-same) * F.relu(config_margin - dist2)\n",
    "\n",
    "        L_contrast = (loss_pos + loss_neg).mean()\n",
    "\n",
    "        # total loss\n",
    "        loss = L_class + config_lambda_c * L_contrast\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}  average loss: {total_loss/len(train_dl):.4f}\")\n",
    "\n",
    "torch.save(model, './experiments/spconv_contrastive_L0-1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrastive loss spconv\n",
    "\n",
    "config_max_epochs = 360\n",
    "config_device = 'cuda'\n",
    "config_margin = 1.0\n",
    "config_lambda_c = 0.01\n",
    "\n",
    "train_dl = DataLoader(train_ds,batch_size=1,shuffle=True, collate_fn=lambda x: x[0])\n",
    "test_dl = DataLoader(test_ds,batch_size=1, collate_fn=lambda x: x[0])\n",
    "\n",
    "model = SPConvModel(\n",
    "    in_dim = 3+12,\n",
    "    num_classes=89,\n",
    "    pn_hidden=64,\n",
    "    super_hidden=64,\n",
    "    pn_dropout=0.1,\n",
    "    super_dropout=0.2,\n",
    "    edge_dim=32,\n",
    "    super_num_conv=3\n",
    ").to(device=config_device)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "for epoch in range(1,config_max_epochs+1):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    for d in tqdm(train_dl,total=len(train_ds), desc=f\"Epoch {epoch:03d}\"):\n",
    "        pos         = d['pos'].to(device=config_device)\n",
    "        point_feats = d['point_feats'].to(device=config_device)\n",
    "        labels      = d['labels'].to(device=config_device)\n",
    "        super_idx   = d['superpoint_idx'].to(device=config_device)\n",
    "        edge_idx    = d['edge_index'].to(device=config_device)\n",
    "\n",
    "        S = super_idx.amax() + 1 \n",
    "        logits, z = model(pos, point_feats, super_idx, edge_idx)\n",
    "        L_class = F.cross_entropy(logits, labels)\n",
    "\n",
    "        # contrastive loss\n",
    "        src, dst = edge_idx\n",
    "        zi, zj = z[src], z[dst]\n",
    "        dist2 = (zi - zj).pow(2).sum(dim=1)\n",
    "        same = (labels[src] == labels[dst]).float() # [E]\n",
    "        # pull\n",
    "        loss_pos = same * dist2\n",
    "        # push\n",
    "        loss_neg = (1-same) * F.relu(config_margin - dist2)\n",
    "\n",
    "        L_contrast = (loss_pos + loss_neg).mean()\n",
    "\n",
    "        # total loss\n",
    "        loss = L_class + config_lambda_c * L_contrast\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}  average loss: {total_loss/len(train_dl):.4f}\")\n",
    "\n",
    "torch.save(model, './experiments/spconv_contrastive_L0-01.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrastive loss spconv\n",
    "\n",
    "config_max_epochs = 360\n",
    "config_device = 'cuda'\n",
    "config_margin = 1.0\n",
    "config_lambda_c = 1.0\n",
    "\n",
    "train_dl = DataLoader(train_ds,batch_size=1,shuffle=True, collate_fn=lambda x: x[0])\n",
    "test_dl = DataLoader(test_ds,batch_size=1, collate_fn=lambda x: x[0])\n",
    "\n",
    "model = SPConvModel(\n",
    "    in_dim = 3+12,\n",
    "    num_classes=89,\n",
    "    pn_hidden=64,\n",
    "    super_hidden=64,\n",
    "    pn_dropout=0.1,\n",
    "    super_dropout=0.2,\n",
    "    edge_dim=32,\n",
    "    super_num_conv=3\n",
    ").to(device=config_device)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "for epoch in range(1,config_max_epochs+1):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    for d in tqdm(train_dl,total=len(train_ds), desc=f\"Epoch {epoch:03d}\"):\n",
    "        pos         = d['pos'].to(device=config_device)\n",
    "        point_feats = d['point_feats'].to(device=config_device)\n",
    "        labels      = d['labels'].to(device=config_device)\n",
    "        super_idx   = d['superpoint_idx'].to(device=config_device)\n",
    "        edge_idx    = d['edge_index'].to(device=config_device)\n",
    "\n",
    "        S = super_idx.amax() + 1 \n",
    "        logits, z = model(pos, point_feats, super_idx, edge_idx)\n",
    "        L_class = F.cross_entropy(logits, labels)\n",
    "\n",
    "        # contrastive loss\n",
    "        src, dst = edge_idx\n",
    "        zi, zj = z[src], z[dst]\n",
    "        dist2 = (zi - zj).pow(2).sum(dim=1)\n",
    "        same = (labels[src] == labels[dst]).float() # [E]\n",
    "        # pull\n",
    "        loss_pos = same * dist2\n",
    "        # push\n",
    "        loss_neg = (1-same) * F.relu(config_margin - dist2)\n",
    "\n",
    "        L_contrast = (loss_pos + loss_neg).mean()\n",
    "\n",
    "        # total loss\n",
    "        loss = L_class + config_lambda_c * L_contrast\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}  average loss: {total_loss/len(train_dl):.4f}\")\n",
    "\n",
    "torch.save(model, './experiments/spconv_contrastive_L1-00.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2bnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
